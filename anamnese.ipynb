{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -qq install --upgrade youtube-dl\n",
    "!pip -qq install git+https://github.com/openai/whisper.git \n",
    "!pip -qq install transformers\n",
    "!pip -qq install sentencepiece\n",
    "!pip -qq install spacy-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import json \n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "import torch\n",
    "import time\n",
    "from os.path import exists as path_exists\n",
    "from pathlib import Path\n",
    "\n",
    "#Spacy \n",
    "import spacy\n",
    "from spacy.tokens import DocBin\n",
    "from spacy.util import minibatch\n",
    "from spacy.training import Example\n",
    "from spacy.scorer import Scorer\n",
    "from spacy.lang.es import Spanish\n",
    "from spacy.pipeline import EntityRuler\n",
    "\n",
    "#huggingface\n",
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import AutoModelForSeq2SeqLM\n",
    "\n",
    "#whisper\n",
    "import whisper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\Documents\\GitHub\\anamnese\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>description</th>\n",
       "      <th>medical_specialty</th>\n",
       "      <th>sample_name</th>\n",
       "      <th>transcription</th>\n",
       "      <th>keywords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>A 23-year-old white female presents with comp...</td>\n",
       "      <td>Allergy / Immunology</td>\n",
       "      <td>Allergic Rhinitis</td>\n",
       "      <td>SUBJECTIVE:,  This 23-year-old white female pr...</td>\n",
       "      <td>allergy / immunology, allergic rhinitis, aller...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Consult for laparoscopic gastric bypass.</td>\n",
       "      <td>Bariatrics</td>\n",
       "      <td>Laparoscopic Gastric Bypass Consult - 2</td>\n",
       "      <td>PAST MEDICAL HISTORY:, He has difficulty climb...</td>\n",
       "      <td>bariatrics, laparoscopic gastric bypass, weigh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Consult for laparoscopic gastric bypass.</td>\n",
       "      <td>Bariatrics</td>\n",
       "      <td>Laparoscopic Gastric Bypass Consult - 1</td>\n",
       "      <td>HISTORY OF PRESENT ILLNESS: , I have seen ABC ...</td>\n",
       "      <td>bariatrics, laparoscopic gastric bypass, heart...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2-D M-Mode. Doppler.</td>\n",
       "      <td>Cardiovascular / Pulmonary</td>\n",
       "      <td>2-D Echocardiogram - 1</td>\n",
       "      <td>2-D M-MODE: , ,1.  Left atrial enlargement wit...</td>\n",
       "      <td>cardiovascular / pulmonary, 2-d m-mode, dopple...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2-D Echocardiogram</td>\n",
       "      <td>Cardiovascular / Pulmonary</td>\n",
       "      <td>2-D Echocardiogram - 2</td>\n",
       "      <td>1.  The left ventricular cavity size and wall ...</td>\n",
       "      <td>cardiovascular / pulmonary, 2-d, doppler, echo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                        description  \\\n",
       "0           0   A 23-year-old white female presents with comp...   \n",
       "1           1           Consult for laparoscopic gastric bypass.   \n",
       "2           2           Consult for laparoscopic gastric bypass.   \n",
       "3           3                             2-D M-Mode. Doppler.     \n",
       "4           4                                 2-D Echocardiogram   \n",
       "\n",
       "             medical_specialty                                sample_name  \\\n",
       "0         Allergy / Immunology                         Allergic Rhinitis    \n",
       "1                   Bariatrics   Laparoscopic Gastric Bypass Consult - 2    \n",
       "2                   Bariatrics   Laparoscopic Gastric Bypass Consult - 1    \n",
       "3   Cardiovascular / Pulmonary                    2-D Echocardiogram - 1    \n",
       "4   Cardiovascular / Pulmonary                    2-D Echocardiogram - 2    \n",
       "\n",
       "                                       transcription  \\\n",
       "0  SUBJECTIVE:,  This 23-year-old white female pr...   \n",
       "1  PAST MEDICAL HISTORY:, He has difficulty climb...   \n",
       "2  HISTORY OF PRESENT ILLNESS: , I have seen ABC ...   \n",
       "3  2-D M-MODE: , ,1.  Left atrial enlargement wit...   \n",
       "4  1.  The left ventricular cavity size and wall ...   \n",
       "\n",
       "                                            keywords  \n",
       "0  allergy / immunology, allergic rhinitis, aller...  \n",
       "1  bariatrics, laparoscopic gastric bypass, weigh...  \n",
       "2  bariatrics, laparoscopic gastric bypass, heart...  \n",
       "3  cardiovascular / pulmonary, 2-d m-mode, dopple...  \n",
       "4  cardiovascular / pulmonary, 2-d, doppler, echo...  "
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(os.getcwd())\n",
    "df = pd.read_csv(r\"C:\\Users\\Admin\\Documents\\GitHub\\anamnese\\content\\archive.zip\", compression='zip', header=0, sep=',', quotechar='\"')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ICI CHANGER LE HELSINKI MODEL POUR PASSER D L ESPAGNOL AU FRANCAIS §§§§§§§§§§§§§§§§§§§§§§§§§§§§§§   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "tokenizer_t = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-en-fr\")\n",
    "\n",
    "model_t = AutoModelForSeq2SeqLM.from_pretrained(\"Helsinki-NLP/opus-mt-en-fr\")\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_t = model_t.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(text: str):\n",
    "    inputs = tokenizer_t.encode(text, return_tensors=\"pt\",padding=True,max_length=512,truncation=True)\n",
    "    inputs = inputs.to(device)\n",
    "    outputs = model_t.generate(inputs,max_length=512, num_beams=5,early_stopping=True)## num_beams=None ## précédent code \n",
    "    return tokenizer_t.decode(outputs[0], skip_special_tokens=True) ## GPT \n",
    "    ##translated = tokenizer_t.decode(outputs[0]).replace('',\"\").replace('',\"\").replace('',\"\").strip().lower()\n",
    "\n",
    "    return translated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4994    Patient ayant une sinusite sévère il y a envir...\n",
       "4995    Il s'agit d'un enfant de 14 mois Caucasien qui...\n",
       "4996    Une femelle pour un physique complet et un sui...\n",
       "4997              La mère dit qu'il sifflait et toussait.\n",
       "4998    Réaction allergique aiguë, étiologie incertain...\n",
       "Name: description, dtype: object"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()['description'].apply(translate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = df[['transcription', 'medical_specialty']]\n",
    "filtered_df.loc[:, 'medical_specialty'] = filtered_df['medical_specialty'].apply(lambda x:str.strip(x))\n",
    "mask = (filtered_df['medical_specialty'] == 'SOAP / Chart / Progress Notes') | \\\n",
    "       (filtered_df['medical_specialty'] == 'Office Notes') | \\\n",
    "       (filtered_df['medical_specialty'] == 'Consult - History and Phy.') | \\\n",
    "       (filtered_df['medical_specialty'] == 'Emergency Room Reports') | \\\n",
    "       (filtered_df['medical_specialty'] == 'Discharge Summary') | \\\n",
    "       (filtered_df['medical_specialty'] == 'Letters')\n",
    "filtered_df = filtered_df[~mask]\n",
    "data_categories  = filtered_df.groupby(filtered_df['medical_specialty'])\n",
    "\n",
    "filtered_data_categories = data_categories.filter(lambda x:x.shape[0] > 50)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "medical_specialty\n",
       "Surgery                       1103\n",
       "Cardiovascular / Pulmonary     372\n",
       "Orthopedic                     355\n",
       "Radiology                      273\n",
       "General Medicine               259\n",
       "Gastroenterology               230\n",
       "Neurology                      223\n",
       "Obstetrics / Gynecology        160\n",
       "Urology                        158\n",
       "ENT - Otolaryngology            98\n",
       "Neurosurgery                    94\n",
       "Hematology - Oncology           90\n",
       "Ophthalmology                   83\n",
       "Nephrology                      81\n",
       "Pediatrics - Neonatal           70\n",
       "Pain Management                 62\n",
       "Psychiatry / Psychology         53\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_data_categories['medical_specialty'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transcription</th>\n",
       "      <th>medical_specialty</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2874</th>\n",
       "      <td>CC:, Dysarthria,HX: ,This 52y/o RHF was transf...</td>\n",
       "      <td>Neurology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2123</th>\n",
       "      <td>EXAM:  ,Thoracic Spine.,REASON FOR EXAM: , Inj...</td>\n",
       "      <td>Orthopedic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2328</th>\n",
       "      <td>PREOPERATIVE DIAGNOSIS: , Recurrent anterior d...</td>\n",
       "      <td>Orthopedic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3533</th>\n",
       "      <td>REASON FOR VISIT: , Followup of laparoscopic f...</td>\n",
       "      <td>Gastroenterology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1614</th>\n",
       "      <td>EXAM:,  Lexiscan Nuclear Myocardial Perfusion ...</td>\n",
       "      <td>Radiology</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          transcription medical_specialty\n",
       "2874  CC:, Dysarthria,HX: ,This 52y/o RHF was transf...         Neurology\n",
       "2123  EXAM:  ,Thoracic Spine.,REASON FOR EXAM: , Inj...        Orthopedic\n",
       "2328  PREOPERATIVE DIAGNOSIS: , Recurrent anterior d...        Orthopedic\n",
       "3533  REASON FOR VISIT: , Followup of laparoscopic f...  Gastroenterology\n",
       "1614  EXAM:,  Lexiscan Nuclear Myocardial Perfusion ...         Radiology"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cleaned = filtered_data_categories.sample(frac=1.0)\n",
    "df_cleaned = df_cleaned.dropna(subset=['transcription'])\n",
    "df_cleaned.head()\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Label the NER in English using the huggingface model for biomedical\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"d4data/biomedical-ner-all\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"d4data/biomedical-ner-all\")\n",
    "\n",
    "pipe = pipeline(\"ner\", model=model, tokenizer=tokenizer, aggregation_strategy=\"simple\", device=device)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_2692\\457871124.py:1: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  tqdm_notebook().pandas()\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72b42d109ae4405ab46b3a7f7d6caabc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98e826f96e904ce09abc1ebf9cb79a55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3732 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tqdm_notebook().pandas()\n",
    "df_cleaned['ner_en']  = df_cleaned['transcription'].progress_apply(pipe)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transcription</th>\n",
       "      <th>medical_specialty</th>\n",
       "      <th>ner_en</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2874</th>\n",
       "      <td>CC:, Dysarthria,HX: ,This 52y/o RHF was transf...</td>\n",
       "      <td>Neurology</td>\n",
       "      <td>[{'entity_group': 'Lab_value', 'score': 0.9996...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2123</th>\n",
       "      <td>EXAM:  ,Thoracic Spine.,REASON FOR EXAM: , Inj...</td>\n",
       "      <td>Orthopedic</td>\n",
       "      <td>[{'entity_group': 'Biological_structure', 'sco...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2328</th>\n",
       "      <td>PREOPERATIVE DIAGNOSIS: , Recurrent anterior d...</td>\n",
       "      <td>Orthopedic</td>\n",
       "      <td>[{'entity_group': 'Detailed_description', 'sco...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3533</th>\n",
       "      <td>REASON FOR VISIT: , Followup of laparoscopic f...</td>\n",
       "      <td>Gastroenterology</td>\n",
       "      <td>[{'entity_group': 'Therapeutic_procedure', 'sc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1614</th>\n",
       "      <td>EXAM:,  Lexiscan Nuclear Myocardial Perfusion ...</td>\n",
       "      <td>Radiology</td>\n",
       "      <td>[{'entity_group': 'Diagnostic_procedure', 'sco...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          transcription medical_specialty  \\\n",
       "2874  CC:, Dysarthria,HX: ,This 52y/o RHF was transf...         Neurology   \n",
       "2123  EXAM:  ,Thoracic Spine.,REASON FOR EXAM: , Inj...        Orthopedic   \n",
       "2328  PREOPERATIVE DIAGNOSIS: , Recurrent anterior d...        Orthopedic   \n",
       "3533  REASON FOR VISIT: , Followup of laparoscopic f...  Gastroenterology   \n",
       "1614  EXAM:,  Lexiscan Nuclear Myocardial Perfusion ...         Radiology   \n",
       "\n",
       "                                                 ner_en  \n",
       "2874  [{'entity_group': 'Lab_value', 'score': 0.9996...  \n",
       "2123  [{'entity_group': 'Biological_structure', 'sco...  \n",
       "2328  [{'entity_group': 'Detailed_description', 'sco...  \n",
       "3533  [{'entity_group': 'Therapeutic_procedure', 'sc...  \n",
       "1614  [{'entity_group': 'Diagnostic_procedure', 'sco...  "
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cleaned.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_specialty_keywords = {}\n",
    "dict_specialty_keywords_fr = {}\n",
    "\n",
    "\n",
    "def create_dict(list_elem):\n",
    "    for elem in list_elem:\n",
    "      if elem['entity_group'] in dict_specialty_keywords:\n",
    "        dict_specialty_keywords[elem['entity_group']].append(elem['word'])\n",
    "        dict_specialty_keywords_fr[elem['entity_group']].append(translate(elem['word']))\n",
    "      else:\n",
    "        dict_specialty_keywords[elem['entity_group']] = [elem['word']]\n",
    "        dict_specialty_keywords_fr[elem['entity_group']] = [translate(elem['word'])]\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_2692\\4269868783.py:1: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  tqdm_notebook().pandas()\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78314980387d4efa9071a510a79ba748",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2217836c3cc4e28a00ac4df75a7dbb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3732 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tqdm_notebook().pandas()\n",
    "\n",
    "_ = df_cleaned['ner_en'].progress_apply(create_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"ner_annotations_fr.json\", 'w') as f:\n",
    "    json.dump(dict_specialty_keywords_fr, f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_2692\\1540691733.py:1: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  tqdm_notebook().pandas()\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab052f7e8ded4925b3ba95a6b5c6c9fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b1b8e2a44294c5c85bbe2640219edf3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3732 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tqdm_notebook().pandas()\n",
    "\n",
    "df_cleaned['transcription_fr'] = df_cleaned['transcription'].progress_apply(translate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned.to_csv(\"df_cleaned.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the annotated dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['Lab_value', 'Diagnostic_procedure', 'Disease_disorder', 'Clinical_event', 'Nonbiological_location', 'Date', 'Detailed_description', 'Sign_symptom', 'Subject', 'Activity', 'Biological_structure', 'Severity', 'Duration', 'Area', 'Therapeutic_procedure', 'Medication', 'Time', 'Age', 'Frequency', 'Dosage', 'Administration', 'Sex', 'History', 'Other_entity', 'Distance', 'Personal_background', 'Coreference', 'Volume', 'Quantitative_concept', 'Color', 'Family_history', 'Shape', 'Occupation', 'Other_event', 'Qualitative_concept', 'Outcome', 'Texture', 'Height'])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_specialty_keywords_fr = json.load(open(\"ner_annotations_fr.json\"))\n",
    "\n",
    "dict_specialty_keywords_fr.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for k in dict_specialty_keywords_fr.keys():\n",
    "  dict_specialty_keywords_fr[k] = set(dict_specialty_keywords_fr[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in dict_specialty_keywords_fr.keys():\n",
    "  dict_specialty_keywords_fr[k] = [el.strip() for el in list(dict_specialty_keywords_fr[k]) if len(el.strip())>=4 or k==\"Age\"]\n",
    "     \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And delete some wrong words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lab_value\n",
      "Diagnostic_procedure\n",
      "Disease_disorder\n",
      "Clinical_event\n",
      "Nonbiological_location\n",
      "Date\n",
      "Detailed_description\n",
      "Sign_symptom\n",
      "Subject\n",
      "Activity\n",
      "Biological_structure\n",
      "Severity\n",
      "Duration\n",
      "Area\n",
      "Therapeutic_procedure\n",
      "Medication\n",
      "Time\n",
      "Age\n",
      "Frequency\n",
      "Dosage\n",
      "Administration\n",
      "Sex\n",
      "History\n",
      "Other_entity\n",
      "Distance\n",
      "Personal_background\n",
      "Coreference\n",
      "Volume\n",
      "Quantitative_concept\n",
      "Color\n",
      "Family_history\n",
      "Shape\n",
      "Occupation\n",
      "Other_event\n",
      "Qualitative_concept\n",
      "Outcome\n",
      "Texture\n",
      "Height\n"
     ]
    }
   ],
   "source": [
    "for category in dict_specialty_keywords_fr.keys():\n",
    "    print(category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for the class sex we delete the following words\n",
    "list_to_delete = ['vétéran', 'dentiste', 'mariés',]\n",
    "\n",
    "for el in list_to_delete: \n",
    "  dict_specialty_keywords_fr['Sex'].pop(dict_specialty_keywords_fr['Sex'].index(el))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['divorce',\n",
       " 'année',\n",
       " \"ne pas fumer, boire de l'alcool\",\n",
       " 'Bien soigné',\n",
       " \"de mariage ou d'enfants\",\n",
       " \"pas d'histoire\",\n",
       " 'néphropathie diabétique',\n",
       " 'glioblastome',\n",
       " 'Activités sexuelles',\n",
       " 'prothèses dentaires',\n",
       " 'refusé',\n",
       " 'coronaire',\n",
       " 'multiples',\n",
       " 'venin',\n",
       " '##lux',\n",
       " 'Voyages ou expositions récents',\n",
       " 'toute intervention médicale antérieure importante',\n",
       " 'bière par jour',\n",
       " 'allergique à la codéine, à la pénicilline',\n",
       " 'DIRECTION',\n",
       " '##se',\n",
       " 'nourris',\n",
       " 'consommer rarement',\n",
       " '# #Ropop',\n",
       " 'collège',\n",
       " 'glissé',\n",
       " 'foie positif',\n",
       " '##Nchospasme',\n",
       " 'Très bien.',\n",
       " 'diabète sucré',\n",
       " \"pas d' antécédents de tuberculose\",\n",
       " 'lourd',\n",
       " '8ème',\n",
       " 'vertiges',\n",
       " 'arrêter',\n",
       " 'hernies',\n",
       " 'médicaments',\n",
       " 'fumées',\n",
       " 'Nous',\n",
       " 'colorant',\n",
       " 'plus jamais',\n",
       " 'ourlet',\n",
       " 'de parkinson',\n",
       " '- semaine',\n",
       " 'hystérectomie totale',\n",
       " '25 ans',\n",
       " 'La tension',\n",
       " 'mules',\n",
       " 'colectomie',\n",
       " 'non-fumeur',\n",
       " 'morbide',\n",
       " 'le tabac refusé',\n",
       " 'une bicyclette',\n",
       " 'houille',\n",
       " \"antécédents de l'artère coronaire\",\n",
       " 'gravida',\n",
       " 'malade',\n",
       " 'retard mental',\n",
       " 'antécédents médicaux importants',\n",
       " 'tabac,',\n",
       " 'non esp',\n",
       " 'melena',\n",
       " 'suicide',\n",
       " '# ##ty',\n",
       " 'Polyarthrite heumatique',\n",
       " '##mes',\n",
       " 'menarche',\n",
       " 'cancer du testicule',\n",
       " 'fumée ou boisson',\n",
       " 'Maladies sexuellement transmissibles',\n",
       " 'néphropathie hiv',\n",
       " 'nouveaux',\n",
       " '50 –',\n",
       " \"pas d'enfants\",\n",
       " 'incarcérés',\n",
       " 'tous produits du tabac',\n",
       " \"nie la consommation d'alcool, illicites\",\n",
       " '##a bifida',\n",
       " '- 41 - semaine',\n",
       " '32 semaines de gestation',\n",
       " '##othyroïde',\n",
       " 'constipation',\n",
       " 'détresse aiguë',\n",
       " 'Codeine',\n",
       " 'consommer',\n",
       " '##bri',\n",
       " 'coups de pied',\n",
       " 'hypothyroïdien',\n",
       " 'céphalées',\n",
       " 'Primigravida',\n",
       " 'côlon',\n",
       " 'rayonnement',\n",
       " 'fille dominante de la main droite',\n",
       " '##tomie',\n",
       " 'ci-dessus',\n",
       " '##ergy',\n",
       " \"C'est pour ça qu'on est là.\",\n",
       " 'État',\n",
       " \"C'est lui.\",\n",
       " 'boisson',\n",
       " '##Ctive pulmonaire',\n",
       " \"naissance de l'enfant\",\n",
       " 'insuffisance rénale terminale',\n",
       " '##smo',\n",
       " 'historien',\n",
       " 'abus de cocaïne',\n",
       " '##heumatique',\n",
       " \"exposition à l'amiante\",\n",
       " 'sexuellement actifs',\n",
       " '29 - 3 / 7 - semaine',\n",
       " 'clip',\n",
       " 'compatible',\n",
       " 'poing',\n",
       " \"d'élevage ou d'élevage d'oiseaux ou chats\",\n",
       " 'problèmes médicaux multiples',\n",
       " 'Hépatite',\n",
       " 'hyperparathy',\n",
       " '##phi',\n",
       " 'apparaissant',\n",
       " 'usage de drogues intraveineuses',\n",
       " 'polyarthrite rhumatoïde',\n",
       " 'pénici',\n",
       " 'étape iii cckd',\n",
       " 'maladies rénales',\n",
       " 'droite - main dominante',\n",
       " 'électrocons',\n",
       " 'cyanose',\n",
       " 'trois',\n",
       " 'pneumonie',\n",
       " '##ru',\n",
       " 'petite amie',\n",
       " 'vésicule biliaire',\n",
       " 'Couper',\n",
       " 'non - fumeur de cigarettes',\n",
       " 'bouche',\n",
       " 'alcool le week-end',\n",
       " 'Tachycardie ventriculaire',\n",
       " '##clairé',\n",
       " \"voyages à l'étranger\",\n",
       " 'athérosclérose',\n",
       " 'sein',\n",
       " 'aicd',\n",
       " 'ambule',\n",
       " 'gouttes',\n",
       " 'régime alimentaire',\n",
       " 'sueur',\n",
       " 'vapeur de peinture',\n",
       " 'ou de loisirs',\n",
       " 'illicites',\n",
       " 'difficultés',\n",
       " 'ne pas fumer de cigarettes, consommer de la drogue',\n",
       " 'actif',\n",
       " '##Rin',\n",
       " 'battus',\n",
       " '##ypothyroïde',\n",
       " \"consomme rarement de l'alcool\",\n",
       " 'carence en fer',\n",
       " 'par. 3',\n",
       " '10 ans',\n",
       " 'asthme',\n",
       " '##ergy aux médicaments sulfa',\n",
       " '##ures',\n",
       " 'rencontré',\n",
       " 'Blessure du globe ouvert',\n",
       " 'mené',\n",
       " 'gravida - 1',\n",
       " 'g − 2',\n",
       " 'État du cholestérol',\n",
       " 'dehors',\n",
       " 'Équitation',\n",
       " \"ne consomme pas d'alcool\",\n",
       " 'fièvre',\n",
       " 'Carcinome tric',\n",
       " 'hypercholestérol',\n",
       " 'pays et',\n",
       " 'aucun historique médical antérieur apparent',\n",
       " 'frappe',\n",
       " 'obst. chronique',\n",
       " 'bulbaire cérébral',\n",
       " 'disques',\n",
       " 'post-ménopausique',\n",
       " 'elle',\n",
       " 'célibataire',\n",
       " 'habillée et soignée de façon décontractée',\n",
       " 'Chondrosarcome',\n",
       " 'arthrose',\n",
       " \"N'est pas\",\n",
       " 'handicap',\n",
       " 'vêtements propres',\n",
       " \"pas d'antécédents de maladie coronarienne établie\",\n",
       " 'dents',\n",
       " 'tabac chronique',\n",
       " 'activité homosexuelle',\n",
       " 'péricardite',\n",
       " 'g7, p5',\n",
       " '##isations',\n",
       " 'connue',\n",
       " 'école',\n",
       " 'relation monogame',\n",
       " '##némoconiose',\n",
       " 'ou de drogues illicites',\n",
       " '##Vent',\n",
       " 'coeur rhumatismal',\n",
       " '##ut attaques',\n",
       " '##fe',\n",
       " '##hmias',\n",
       " '##alt',\n",
       " '# ##té',\n",
       " 'douteux',\n",
       " 'positif',\n",
       " 'fièvre écarlate',\n",
       " '##utation',\n",
       " 'haltérophilie',\n",
       " 'normoc',\n",
       " '##be',\n",
       " 'bouteille alimentée',\n",
       " 'à bas',\n",
       " 'deux paquets par jour',\n",
       " 'essai',\n",
       " 'fasciite plantaire',\n",
       " '##cystectomie',\n",
       " 'utilisant la caféine',\n",
       " 'troubles convulsions',\n",
       " 'Poste',\n",
       " \"brûlures d'estomac\",\n",
       " 'exposition sexuelle',\n",
       " 'main',\n",
       " 'dépendance',\n",
       " 'Voyages',\n",
       " 'pcos',\n",
       " 'Chirurgie',\n",
       " '##laucom',\n",
       " 'Loisirs',\n",
       " \"consommation d'alcool, de tabac ou de drogues illicites\",\n",
       " 'la maladie de la hodgkine',\n",
       " 'o2 dépendance',\n",
       " \"pas d' antécédents médicaux significatifs\",\n",
       " 'Il mange bien.',\n",
       " 'transfusions sanguines multiples',\n",
       " 'élargie',\n",
       " 'rhuma',\n",
       " 'flic',\n",
       " 'cholestérol élevé',\n",
       " \"20 pack - histoire de l'année\",\n",
       " 'Utilisation',\n",
       " 'peu de boissons',\n",
       " 'g2, p0',\n",
       " '- stade rénal',\n",
       " 'ceinture de sécurité',\n",
       " '1940',\n",
       " \"nie la consommation de tabac ou d'alcool\",\n",
       " 'et le paquet droit',\n",
       " \"stade de l'insuffisance rénale\",\n",
       " 'perte visuelle',\n",
       " 'paquets de cigarettes par jour',\n",
       " 'drogues illicites/veineuses',\n",
       " 'affaire',\n",
       " 'dans',\n",
       " 'boissons alcooliques peu fréquentes',\n",
       " 'fumée de cigarette',\n",
       " 'grossesse',\n",
       " 'diabétique dépendant mel',\n",
       " 'allergique au keflex',\n",
       " 'mentalement malades',\n",
       " 'toute la vie',\n",
       " 'cœur',\n",
       " 'défaillance',\n",
       " 'migraine',\n",
       " 'constipati',\n",
       " '##tas',\n",
       " 'doré',\n",
       " 'par. 1 - 0 - 5 - 1',\n",
       " \"sans beaucoup d'histoires médicales passées\",\n",
       " \"tabagisme, abus d'alcool\",\n",
       " '# ##ative',\n",
       " 'multiples g',\n",
       " \"Il n'y a pas de\",\n",
       " 'Cancer de la peau',\n",
       " \"refus d'alcool, tabac\",\n",
       " \"d'abus de drogues\",\n",
       " \"nie la consommation de tabac, d'alcool ou de drogues\",\n",
       " 'a bien réussi',\n",
       " 'agités',\n",
       " 'endosco',\n",
       " 'alcool, tabac',\n",
       " \"l'abus de tabac\",\n",
       " '##dysmorp',\n",
       " 'Voie intraveineuse',\n",
       " '##ctomie',\n",
       " '##othyroïdisme',\n",
       " '148 livres',\n",
       " 'emballage par jour',\n",
       " '##tri',\n",
       " 'six bières',\n",
       " 'de la maladie coronarienne',\n",
       " 'menstruelles',\n",
       " 'cesser de fumer',\n",
       " '##agic',\n",
       " '## abus de substances',\n",
       " 'résiliation',\n",
       " 'ulcères',\n",
       " '##iter',\n",
       " 'Annexe',\n",
       " 'AVC, diabète',\n",
       " 'une année',\n",
       " \"##infarctus de l'ocardie\",\n",
       " 'Pas de tabac',\n",
       " 'poste',\n",
       " 'abus sexuels',\n",
       " 'activités de la vie quotidienne',\n",
       " 'Traumatisme',\n",
       " 'lunettes',\n",
       " 'maladie du chateau',\n",
       " 'obésité',\n",
       " 'arthrite diffuse',\n",
       " '##ee',\n",
       " \"dans l'incapacité\",\n",
       " 'sueur nocturne',\n",
       " 'véhicule',\n",
       " 'précédent mi',\n",
       " 'chirurgie',\n",
       " 'consommation de drogues',\n",
       " 'alcoolisme',\n",
       " 'antérieure',\n",
       " 'nouveau port',\n",
       " \"pas d'important passé médical ou\",\n",
       " 'cholestérol',\n",
       " 'pulmonaire',\n",
       " 'problèmes',\n",
       " 'hypertension, diabète sucré, fièvre rhumatismale',\n",
       " '##rombocythémie',\n",
       " 'accident de voiture',\n",
       " 'occasionnel',\n",
       " 'les joggings',\n",
       " 'vision',\n",
       " '##omy',\n",
       " 'Problèmes génito-urinaires',\n",
       " 'passé',\n",
       " 'Tatouages',\n",
       " 'Restriction',\n",
       " \"Cancer de l'urine\",\n",
       " \"pas d'alcool, de drogue\",\n",
       " 'passé connu',\n",
       " '- 0 - 2',\n",
       " '##phaden',\n",
       " 'Il y a des années',\n",
       " 'consommation de cigarettes',\n",
       " 'nodosa,',\n",
       " \"l'art\",\n",
       " '##uits',\n",
       " 'intra-',\n",
       " 'alcoolique',\n",
       " 'cellulite',\n",
       " 'etoh',\n",
       " 'étudiant',\n",
       " 'iv) Le Conseil de sécurité',\n",
       " '# #Bet',\n",
       " '##ginal',\n",
       " 'période menstruelle',\n",
       " 'pas de fumée ni de boisson',\n",
       " \"pas d' antécédents d'arythmies\",\n",
       " 'travailleur postal retraité',\n",
       " '- et - un demi-jour',\n",
       " 'Montant forfaitaire',\n",
       " 'cancer',\n",
       " '##té',\n",
       " 'allergique à',\n",
       " 'gauche',\n",
       " 'hyperlipide',\n",
       " 'hyperthyroïdien',\n",
       " 'p5 - 0 - 0 - 5',\n",
       " 'tabac',\n",
       " 'abus de drogues',\n",
       " 'faible',\n",
       " 'hépatite c',\n",
       " 'dehors en tant que',\n",
       " 'Disque ##té',\n",
       " '##ds',\n",
       " 'Médicaments à domicile',\n",
       " 'deuxième année',\n",
       " 'froid',\n",
       " 'non pondérale',\n",
       " 'gentleman dominant à droite',\n",
       " \"syndrome d' asplénie\",\n",
       " 'prend de la viande',\n",
       " '##vita',\n",
       " 'avortement spontané',\n",
       " 'veuves',\n",
       " '##maladie pulmonaire ctive',\n",
       " 'enceinte',\n",
       " '##Capacité',\n",
       " 'Forte',\n",
       " 'tonnes',\n",
       " '- 2 - 0 - 0 - 2',\n",
       " 'Sodas',\n",
       " '##Malheur',\n",
       " 'bipolaire',\n",
       " 'maladie vasculaire',\n",
       " '##atique',\n",
       " 'avortements spontanés',\n",
       " 'Saisie',\n",
       " \"d'alcool\",\n",
       " 'Royaume Uni',\n",
       " '##périodes stratégiques',\n",
       " '##laucome',\n",
       " 'ex - fumée',\n",
       " 'acouphènes',\n",
       " 'agité',\n",
       " '50 ans',\n",
       " 'dominante de main',\n",
       " 'péché',\n",
       " 'pénicilline',\n",
       " 'la violence',\n",
       " 'surdosage',\n",
       " 'pas de médicaments',\n",
       " '##otid',\n",
       " 'appendectomie enfantine',\n",
       " 'il y a',\n",
       " '##coce',\n",
       " 'buveur lourd',\n",
       " 'consommation de tabac',\n",
       " 'diète diabétique',\n",
       " 'ou du tabac',\n",
       " \"##période d'essai\",\n",
       " 'dépression',\n",
       " 'gauche - main',\n",
       " 'Problèmes',\n",
       " 'une journée',\n",
       " \"interdit l'usage de cigarettes, d'alcool ou de drogues\",\n",
       " 'par. 0 - 0 - 1 - 0',\n",
       " 'et dyslipidémie',\n",
       " 'psychose',\n",
       " 'Taux de chômage',\n",
       " \"Pas d'opération chirurgicale\",\n",
       " \"l'hématotéchésie\",\n",
       " \"pas d'histoires médicales substantielles\",\n",
       " 'consomme du café',\n",
       " 'sucré',\n",
       " '7e année',\n",
       " \"C'est pas vrai.\",\n",
       " 'humide',\n",
       " 'frère',\n",
       " 'fibromyalgie',\n",
       " 'canule nasale',\n",
       " \"L'artère\",\n",
       " 'gestation',\n",
       " 'boissons',\n",
       " 'exposition aux drogues',\n",
       " \"L'obésité chez les adultes\",\n",
       " 'État après la chute',\n",
       " '##lytique',\n",
       " 'assurance',\n",
       " 'rhinite allergique',\n",
       " 'ordonnance',\n",
       " 'consommer des drogues illicites',\n",
       " 'apnée obstructive du sommeil',\n",
       " 'changement des habitudes intestinales',\n",
       " '##Ooter',\n",
       " 'schizophrénie',\n",
       " '##avida - 0',\n",
       " 'Affections de la thyroïde',\n",
       " '##Lecyste',\n",
       " '##hmia',\n",
       " 'admis',\n",
       " \"À l'aide\",\n",
       " \"J'ai foiré.\",\n",
       " 'des choses',\n",
       " 'Lève-toi.',\n",
       " 'usagers de drogues',\n",
       " \"allergie à l'iode\",\n",
       " 'prostate',\n",
       " 'néph',\n",
       " 'pièce',\n",
       " 'ex-fumeur',\n",
       " 'crevettes',\n",
       " 'non diabétiques',\n",
       " 'sang',\n",
       " \"maladie d'alzheimer\",\n",
       " 'amygdalectomie',\n",
       " 'mauvais traitements',\n",
       " 'de nombreuses années',\n",
       " '##eta',\n",
       " 'circoncision',\n",
       " 'Enfants prématurés de 27 semaines',\n",
       " 'diabète sucré,',\n",
       " 'ex - 32 semaines',\n",
       " 'santé',\n",
       " 'diplômé du secondaire',\n",
       " ', p1',\n",
       " 'neuvième année',\n",
       " 'toux',\n",
       " '250 livres',\n",
       " 'Tabac à main',\n",
       " 'saignement',\n",
       " 'moteur',\n",
       " 'usure',\n",
       " \"consommation de tabac ou d'éthanol\",\n",
       " '##xia',\n",
       " 'abusés',\n",
       " '##sis',\n",
       " 'casque',\n",
       " 'grossesse à terme',\n",
       " 'négatif',\n",
       " 'non compliqué',\n",
       " '##ction',\n",
       " 'mots',\n",
       " \"interprète de l'Ouest\",\n",
       " 'Hémorragie',\n",
       " 'sur le handicap',\n",
       " \"d'alzheimer '\",\n",
       " '- 3 - 0 - 1 - 3',\n",
       " 'éclamps',\n",
       " 'lancers',\n",
       " 'enregistrés',\n",
       " 'alcool',\n",
       " 'non remarquable',\n",
       " 'transmis',\n",
       " 'std historique',\n",
       " '##ang',\n",
       " '##ses',\n",
       " 'tabac, alcool',\n",
       " 'carcinome',\n",
       " \"cancer de l'utérus\",\n",
       " 'Moins de 24 heures',\n",
       " '##iparèse',\n",
       " 'Aucun problème',\n",
       " 'présente',\n",
       " 'éducation',\n",
       " 'saison',\n",
       " 'un demi-paquet',\n",
       " 'pleine -',\n",
       " 'drogues illicites',\n",
       " 'partenaire sexuel',\n",
       " 'cervical',\n",
       " 'éruption cutanée',\n",
       " 'arthrite',\n",
       " 'cigarettes',\n",
       " '##ph',\n",
       " 'grand public',\n",
       " 'hypertension artérielle',\n",
       " 'gradué',\n",
       " 'cigarettes fumées',\n",
       " 'tabac lourd',\n",
       " \"l'enfance\",\n",
       " 'Troubles affectifs bipolaires',\n",
       " 'maladie de la thyroïde',\n",
       " '##Moto',\n",
       " \"tabac d'occasion\",\n",
       " 'sur une roue',\n",
       " 'pancréatite aiguë',\n",
       " 'Croh',\n",
       " 'opposé - sexe',\n",
       " 'Plaintes',\n",
       " 'diabète, arthrose',\n",
       " 'fumer',\n",
       " 'dépend',\n",
       " '##opathie',\n",
       " 'perte de poids',\n",
       " 'cigarettes fumant',\n",
       " 'laissé tomber',\n",
       " 'hypothyre',\n",
       " 'allergique',\n",
       " 'ménopause',\n",
       " 'capitaux',\n",
       " 'main droite',\n",
       " '1994',\n",
       " 'État de la maladie coronarienne',\n",
       " 'drogues récréatives',\n",
       " '- Oui.',\n",
       " '##esthésie',\n",
       " 'droite',\n",
       " 'diabète de type II',\n",
       " 'Aides',\n",
       " 'Activités',\n",
       " 'enseigner',\n",
       " 'une petite serviette sur la tête',\n",
       " 'un paquet par jour',\n",
       " '##urose',\n",
       " 'fréquent',\n",
       " 'hépatite alcoolique',\n",
       " 'gravi',\n",
       " 'type',\n",
       " \"et de boire de l'alcool\",\n",
       " 'coups de poing',\n",
       " 'encoprésis',\n",
       " 'produits du tabac',\n",
       " \"C'est vrai\",\n",
       " '##tum',\n",
       " 'Alerte',\n",
       " 'drogues',\n",
       " 'par.',\n",
       " 'crise aplastique',\n",
       " 'Décomposition',\n",
       " 'porté',\n",
       " 'concurrence',\n",
       " 'péricardectomie',\n",
       " 'hernie ventrale',\n",
       " 'pieds',\n",
       " '# ##und',\n",
       " 'tous',\n",
       " 'cirrhose alcoolique',\n",
       " 'Démence précoce',\n",
       " 'Pas de tia',\n",
       " 'flexion',\n",
       " 'myélome multiple',\n",
       " 'années',\n",
       " '##va',\n",
       " 'boire',\n",
       " \"consommation de tabac, d'alcool ou de drogues illicites\",\n",
       " 'tête',\n",
       " 'contusion',\n",
       " 'ou sueur de nuit',\n",
       " 'usager du tabac',\n",
       " 'véhicule à moteur',\n",
       " 'aucun antécédent de diabète sucré',\n",
       " 'brûlures',\n",
       " 'consommation illégale de drogues',\n",
       " 'néphropathie de reflux',\n",
       " 'décharge',\n",
       " 'goutte',\n",
       " 'convulsions',\n",
       " 'Liga',\n",
       " 'leucémie',\n",
       " 'de bière par',\n",
       " '##hétérisation',\n",
       " '##erine',\n",
       " 'avortement',\n",
       " 'asthme, allergies',\n",
       " 'Entraînementritable',\n",
       " 'ostéoporo',\n",
       " 'g1, p0,',\n",
       " 'fumeur',\n",
       " 'consommation illicite de drogues',\n",
       " 'historique',\n",
       " 'saignements de nez',\n",
       " 'réparation',\n",
       " ', tabac,',\n",
       " 'en pot',\n",
       " 'autisme',\n",
       " '##riorétinite',\n",
       " 'calculs biliaires',\n",
       " 'agressions sexuelles',\n",
       " 'maladie',\n",
       " 'démenti',\n",
       " 'Grossesse utérine',\n",
       " 'Membrane',\n",
       " 'Pousse-toi',\n",
       " '1 - 0 - 1 - 1',\n",
       " 'enlèvement',\n",
       " 'Exposition',\n",
       " 'Capacité',\n",
       " 'aller',\n",
       " '16 ans',\n",
       " '# # #incinéré',\n",
       " 'hépatite infectieuse',\n",
       " 'hiv infecté',\n",
       " 'arthrite pauciarticulaire',\n",
       " 'cholecystect',\n",
       " 'allergique r',\n",
       " 'maux de dents',\n",
       " 'non buvable',\n",
       " 'sexuelle',\n",
       " 'insuline',\n",
       " 'alcool, cigarettes',\n",
       " \"dépendance chronique à l'alcool\",\n",
       " \"C'est ça.\",\n",
       " '##Teneurs',\n",
       " 'épilepsie',\n",
       " 'hématome du foie',\n",
       " 'insuline élevée',\n",
       " '##est',\n",
       " 'médicament',\n",
       " 'arme de poing',\n",
       " 'diabétique',\n",
       " \"n'importe quelle histoire à travers\",\n",
       " 'cheville cassée',\n",
       " 'abus de substances',\n",
       " 'hyper',\n",
       " '## Habitudes ininaires',\n",
       " 'né(e)',\n",
       " 'de même sexe',\n",
       " \"La maladie de l'artère\",\n",
       " 'périphérique vasculaire',\n",
       " 'Céphalospor',\n",
       " \"boire de l'alcool\",\n",
       " 'retraités',\n",
       " 'perte',\n",
       " 'exposition à la fumée',\n",
       " 'significatif',\n",
       " 'fait',\n",
       " 'non-smo',\n",
       " '5 boissons',\n",
       " 'fumée active',\n",
       " 'amiante',\n",
       " '##psychotique',\n",
       " 'vase',\n",
       " \"boire beaucoup d'alcool\",\n",
       " 'pepsi - conducteur de cola',\n",
       " 'vieux',\n",
       " 'boissons alcoolisées par jour',\n",
       " '##heumatoïde',\n",
       " 'piste de pays',\n",
       " 'chômeurs',\n",
       " '##litus',\n",
       " 'deux',\n",
       " 'enfance abusive',\n",
       " 'cigarette',\n",
       " 'maladies transmissibles',\n",
       " 'congé',\n",
       " 'septorhinoplastie',\n",
       " 'accident de véhicule',\n",
       " \"consommation d'alcool\",\n",
       " 'déplacé',\n",
       " 'nie toute histoire chirurgicale significative',\n",
       " 'Hiatal elle',\n",
       " 'conservation',\n",
       " 'préménopausées',\n",
       " 'dominante',\n",
       " 'alcool rare',\n",
       " '##bétique',\n",
       " 'Ne pas fumer',\n",
       " 'cardiomyopathie',\n",
       " 'numéros de téléphone',\n",
       " 'drogué',\n",
       " 'pancréatite chronique',\n",
       " 'approprié',\n",
       " 'paquet et demi de cigarettes par jour',\n",
       " 'vie quotidienne',\n",
       " 'maladies infantiles',\n",
       " 'aller-retour',\n",
       " 'lunettes de lecture',\n",
       " 'p * * femelle',\n",
       " 'héroïne',\n",
       " '##Maman',\n",
       " \"J'en ai assez.\",\n",
       " 'Chlamydia',\n",
       " \"ou de boire de l'alcool\",\n",
       " '##Tomégalie',\n",
       " 'hépatite c positive',\n",
       " 'fibrillation auriculaire',\n",
       " 'Troubles schizoaffectifs',\n",
       " 'Activité sexuelle',\n",
       " '##lau',\n",
       " 'nsvd',\n",
       " \"l'a fait\",\n",
       " 'cocaïne',\n",
       " 'gravida 7',\n",
       " '##cer',\n",
       " 'syncope',\n",
       " 'nie le tabac',\n",
       " 'fumée, boisson',\n",
       " 'frappé',\n",
       " 'basketball',\n",
       " 'type ii',\n",
       " 'accident',\n",
       " 'boisson alcoolique',\n",
       " 'jambes agitées',\n",
       " '- mains',\n",
       " 'altercation',\n",
       " \"N'importe quoi.\",\n",
       " 'allergies',\n",
       " \"C'est vrai.\",\n",
       " '##onie',\n",
       " 'guerre du Vietnam',\n",
       " 'chats',\n",
       " 'droit dominant - main',\n",
       " 'frotter',\n",
       " '###ial',\n",
       " 'application',\n",
       " \"abus d'alcool\",\n",
       " 'mariés',\n",
       " '##océphalique',\n",
       " 'Maladie coronarienne',\n",
       " 'quatre fois',\n",
       " '##Mellit de diabète stationnaire',\n",
       " 'Faites',\n",
       " '##inates',\n",
       " '##stru',\n",
       " \", l'adolescence et l'âge adulte précoce\",\n",
       " 'hygiène',\n",
       " 'Dentifrice mobile',\n",
       " '/ o AVC',\n",
       " 'maladie coronarienne établie',\n",
       " 'neurone diabétique',\n",
       " 'environ',\n",
       " 'marijuana',\n",
       " 'thyroïde',\n",
       " 'Bien auparavant.',\n",
       " 'enfant',\n",
       " ', p0',\n",
       " '##terol',\n",
       " 'complications',\n",
       " \"C'est bon.\",\n",
       " 'terme',\n",
       " 'pas de ménorragie',\n",
       " '##Othyr',\n",
       " 'hommes',\n",
       " \"consommation d'alcool et de drogues\",\n",
       " 'trouble de la coagulation',\n",
       " '## les',\n",
       " 'hyperthyroïdie',\n",
       " 'accident automobile',\n",
       " 'Jumelles',\n",
       " '###ille',\n",
       " 'éthanol',\n",
       " 'est tombé',\n",
       " 'Il y a 20 ans',\n",
       " 'hospitalisations',\n",
       " 'Non.',\n",
       " '1982',\n",
       " 'Pas de fumeurs',\n",
       " 'crampes musculaires',\n",
       " 'texas',\n",
       " 'nouri',\n",
       " 'transfusions multiples',\n",
       " '##gia',\n",
       " 'Nourriture',\n",
       " 'par. 1 - 0 - 0 - 1',\n",
       " '##lipide',\n",
       " 'tombé',\n",
       " '##émie',\n",
       " 'cataractes',\n",
       " 'Cendres',\n",
       " 'hyperlipidémie',\n",
       " 'Échéance',\n",
       " 'Maur',\n",
       " 'adulte',\n",
       " 'transfusion sanguine',\n",
       " 'drogues pour les loisirs',\n",
       " 'haiti',\n",
       " 'Travaux',\n",
       " 'petit ami',\n",
       " 'diabète',\n",
       " 'obèse morbide',\n",
       " '##co',\n",
       " \"ou d'utiliser des drogues illicites\",\n",
       " '##tro',\n",
       " 'papules anormales',\n",
       " 'annexe',\n",
       " 'orientées vers',\n",
       " 'Poids',\n",
       " 'maladies pulmonaires chroniques',\n",
       " 'tuberculose',\n",
       " \"consommer de l'alcool\",\n",
       " 'vivant',\n",
       " 'sommeil',\n",
       " 'Réf.',\n",
       " 'fumée',\n",
       " \"Har d'or\",\n",
       " \"s'étouffer\",\n",
       " 'buveur social',\n",
       " 'mali',\n",
       " 'rare',\n",
       " 'jour',\n",
       " 'État après cabineg',\n",
       " 'mari',\n",
       " 'éruptions cutanées',\n",
       " 'limitée',\n",
       " 'remplacement',\n",
       " 'droite - main',\n",
       " 'extraction de la cataracte',\n",
       " 'Participation',\n",
       " '##moral',\n",
       " 'non transmissibles',\n",
       " 'cigarettes par jour',\n",
       " '##hal',\n",
       " 'boissons alcoolisées',\n",
       " 'sain',\n",
       " 'consommation de drogues à des fins récréatives',\n",
       " 'Mauvais',\n",
       " 'Exposition à la silice',\n",
       " 'cafés',\n",
       " 'Sexe',\n",
       " 'Hépatite a',\n",
       " \"nie la consommation de tabac, d'alcool et de drogues\",\n",
       " 'hypothyroïdie',\n",
       " 'artère coronaire',\n",
       " 'effet bipolaire',\n",
       " 'Hydroélectricité',\n",
       " 'Produits alimentaires de la mer',\n",
       " 'boissons 4 - 8 bières par nuit',\n",
       " '###thy',\n",
       " \"consommation de tabac, d'alcool ou de drogues\",\n",
       " '##date',\n",
       " 'chronique',\n",
       " 'personnel',\n",
       " 'polysubstance',\n",
       " 'taches',\n",
       " 'hépatite b',\n",
       " '##ucinations',\n",
       " 'occasionnellement',\n",
       " 'éveillé',\n",
       " 'stimulateur cardiaque',\n",
       " 'une à',\n",
       " 'de cigarettes par jour',\n",
       " 'deux bières par jour']"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_specialty_keywords_fr['History']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for the class History we delete the following words\n",
    "list_to_delete =[ '25 ans']\n",
    "\n",
    "for el in list_to_delete: \n",
    "  dict_specialty_keywords_fr['History'].pop(dict_specialty_keywords_fr['History'].index(el))\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "'Dr X' is not in list",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[131], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m list_to_delete \u001b[38;5;241m=\u001b[39m [   \u001b[38;5;124m'\u001b[39m\u001b[38;5;124métape\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDr X\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m el \u001b[38;5;129;01min\u001b[39;00m list_to_delete: \n\u001b[1;32m----> 4\u001b[0m   dict_specialty_keywords_fr[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNonbiological_location\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mpop(dict_specialty_keywords_fr[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNonbiological_location\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mindex(el))\n",
      "\u001b[1;31mValueError\u001b[0m: 'Dr X' is not in list"
     ]
    }
   ],
   "source": [
    "#for the class Nonbiological_location we delete the following words\n",
    "list_to_delete = [ 'Dr X']\n",
    "for el in list_to_delete: \n",
    "  dict_specialty_keywords_fr['Nonbiological_location'].pop(dict_specialty_keywords_fr['Nonbiological_location'].index(el))\n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the class Age\n",
    "list_to_delete = ['seul', 'moyen', 'et', ',', 'à', 'kg', '1812 g', 'école', 'regardant', '## poids', 'ou']\n",
    "\n",
    "for el in list_to_delete: \n",
    "  try:\n",
    "    dict_specialty_keywords_fr['Age'].pop(dict_specialty_keywords_fr['Age'].index(el))\n",
    "  except:\n",
    "    continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_to_delete = ['Quel est le sens de la bible ?', 'admission', 'vieux', 'couple', 'autres', 'quelques', 'peu après',\n",
    " 'dedans', 'autres', 'âge 92', 'peut', 'un jour hors de l’école', 'nuit', 'âge de 12 ans', 'les premiers 4', '8 semaines d’âge gestationnel', 'jour de',\n",
    " 'âge de 42 ans', 'peut', 'cette fois', 'plusieurs', '6 ans', 'vieux', 'âge de 75 ans', '37.5 semaines de gestation', '20 ans', '20 ans', '89 ans', '31 mois', 'vie', 'plus tard dans la journée', 'presque', 'poste', 'certains', 'Quel est le problème ?', 'ceci', 'fait']\n",
    "\n",
    "for el in list_to_delete: \n",
    "  try:\n",
    "    dict_specialty_keywords_fr['Date'].pop(dict_specialty_keywords_fr['Date'].index(el))\n",
    "  except:\n",
    "    continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_to_delete = ['riche', 'acquiescer', 'tubes', 'L4 - 5', 'voix réelle', 'multiple', 'arrière', 'oui, bien sûr.', 'oui, bien sûr.',\n",
    " 'processus', 'jambon', '- L4', 'tapis', 'et aînés', 'ajouter', 'courroie', 'court', 'chaque', 'armes',\n",
    " 'pour', 'droits de l’homme', 'péché', 'basilic', 'et chants latéraux', 'un point', 'L1 - 2', 'faux',\n",
    " 'repos', 'gomme à mâcher', 'dans le', 'peu', 'bien', 'entre', 'de la', 'un', 'Comme']\n",
    "\n",
    "for el in list_to_delete: \n",
    "  try:\n",
    "    dict_specialty_keywords_fr['Biological_structure'].pop(dict_specialty_keywords_fr['Biological_structure'].index(el))\n",
    "  except:\n",
    "    continue\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_to_delete = ['dehors !', 'carte', 'dans le', 'solution saline', 'chapitre', 'sombre', 'nuit', 'coton', 'en vrac', 'clair',\n",
    " 'recherche de mots', 'travaillé', 'dehors', 'photo', 'capable de', 'respiration', 'frappé', 'pertes sur le terrain',\n",
    " 'tournoyant', 'l’a fait', 'stabilisé', 'L5 -', 'niveau', 'navire', 'force', 'voyage', 'chair', 'fonction',\n",
    " 'architecture du sommeil', 'jaune', 'actif', 'signal de câble', 'journal', 'dentelle', 'résoudre', 'talc']\n",
    "\n",
    "for el in list_to_delete: \n",
    "  try:\n",
    "    dict_specialty_keywords_fr['Sign_symptom'].pop(dict_specialty_keywords_fr['Sign_symptom'].index(el))\n",
    "  except:\n",
    "    continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_to_delete = ['peut', 'difficile', 'moins', 'partie', 'comme', 'premier']\n",
    "\n",
    "for el in list_to_delete: \n",
    "  try:\n",
    "    dict_specialty_keywords_fr['Detailed_description'].pop(dict_specialty_keywords_fr['Detailed_description'].index(el))\n",
    "  except:\n",
    "    continue\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_to_delete = ['Dans le', 'peut', 'a', 'lieu']\n",
    "\n",
    "for el in list_to_delete: \n",
    "  try:\n",
    "    dict_specialty_keywords_fr['Therapeutic_procedure'].pop(dict_specialty_keywords_fr['Therapeutic_procedure'].index(el))\n",
    "  except:\n",
    "    continue\n",
    "\n",
    "# dict_specialty_keywords_fr['Therapeutic_procedure']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "list_to_delete =['pour', 'vivant', 'trois', '- oui.', 'cultivé', 'un 18', 'sept livres', 'carte', 'chambre', 'plus âgé', 'aînés',\n",
    "                 'âge', 'approuvées', 'début', 'peu', 'fertile', 'quatrième']\n",
    "\n",
    "for el in list_to_delete: \n",
    "  try:\n",
    "    dict_specialty_keywords_fr['Subject'].pop(dict_specialty_keywords_fr['Subject'].index(el))\n",
    "  except:\n",
    "    continue\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we delete some keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "list_delete = ['Frequency', 'Quantitative_concept',  'Coreference', 'Distance', 'Time', 'Area','Administration',\n",
    "               'Other_entity', 'Volume', 'Shape', 'Texture', 'Other_event', 'Height','Lab_value', 'Duration']\n",
    "\n",
    "for elem in list_delete:\n",
    "  del dict_specialty_keywords_fr[elem] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the ruler ner model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RulerModel():\n",
    "    def __init__(self, dict_specialty_keywords):\n",
    "        self.ruler_model = spacy.blank('es')\n",
    "        self.entity_ruler =  self.ruler_model.add_pipe('entity_ruler')\n",
    "        \n",
    "        total_patterns = []\n",
    "\n",
    "        for k,v in dict_specialty_keywords_fr.items():\n",
    "        \n",
    "          patterns = self.create_patterns(v, k)\n",
    "          total_patterns.extend(patterns)\n",
    "       \n",
    "        self.add_patterns_into_ruler(total_patterns)\n",
    "        \n",
    "       \n",
    "    def create_patterns(self, entity_type_set, entity_type):\n",
    "        patterns = []\n",
    "        for item in entity_type_set:\n",
    "            pattern = {'label': entity_type, 'pattern': item}\n",
    "            patterns.append(pattern)\n",
    "        return patterns\n",
    "    \n",
    "    def add_patterns_into_ruler(self, total_patterns):\n",
    "        self.entity_ruler.add_patterns(total_patterns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and the dataset generator\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenerateDataset(object):\n",
    "    \n",
    "    def __init__(self, ruler_model):\n",
    "        self.ruler_model = ruler_model.ruler_model\n",
    "        \n",
    "    def find_entitytypes(self, text):\n",
    "        ents = [] \n",
    "        doc = self.ruler_model(str(text))\n",
    "        for ent in doc.ents:\n",
    "            ents.append((ent.start_char, ent.end_char, ent.label_))\n",
    "        return ents     \n",
    "    \n",
    "    def assign_labels_to_documents(self, df):\n",
    "        dataset = []\n",
    "        text_list = df['transcription_fr'].values.tolist()\n",
    "        for text in text_list:\n",
    "            ents = self.find_entitytypes(text)\n",
    "            if len(ents) > 0:\n",
    "                dataset.append((text, {'entities': ents}))\n",
    "            else:\n",
    "                continue \n",
    "        return dataset\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Unnamed: 0', 'description', 'medical_specialty', 'sample_name',\n",
      "       'transcription', 'keywords'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df_cleaned.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "ruler_ner = RulerModel(dict_specialty_keywords_fr)\n",
    "ds_ner = GenerateDataset(ruler_ner)\n",
    "df_cleaned = pd.read_csv(r\"C:\\Users\\Admin\\Documents\\GitHub\\anamnese\\content\\archive.zip\", compression='zip', header=0, sep=',', quotechar='\"')## GPT\n",
    "df_cleaned.rename(columns={'transcription': 'transcription_fr'}, inplace=True)\n",
    "\n",
    "df_ner = ds_ner.assign_labels_to_documents(df_cleaned)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the dataset generated:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('SUBJECTIVE:,  This 23-year-old white female presents with complaint of allergies.  She used to have allergies when she lived in Seattle but she thinks they are worse here.  In the past, she has tried Claritin, and Zyrtec.  Both worked for short time but then seemed to lose effectiveness.  She has used Allegra also.  She used that last summer and she began using it again two weeks ago.  It does not appear to be working very well.  She has used over-the-counter sprays but no prescription nasal sprays.  She does have asthma but doest not require daily medication for this and does not think it is flaring up.,MEDICATIONS: , Her only medication currently is Ortho Tri-Cyclen and the Allegra.,ALLERGIES: , She has no known medicine allergies.,OBJECTIVE:,Vitals:  Weight was 130 pounds and blood pressure 124/78.,HEENT:  Her throat was mildly erythematous without exudate.  Nasal mucosa was erythematous and swollen.  Only clear drainage was seen.  TMs were clear.,Neck:  Supple without adenopathy.,Lungs:  Clear.,ASSESSMENT:,  Allergic rhinitis.,PLAN:,1.  She will try Zyrtec instead of Allegra again.  Another option will be to use loratadine.  She does not think she has prescription coverage so that might be cheaper.,2.  Samples of Nasonex two sprays in each nostril given for three weeks.  A prescription was written as well.',\n",
       " {'entities': [(71, 80, 'Sign_symptom'),\n",
       "   (100, 109, 'Sign_symptom'),\n",
       "   (491, 496, 'Detailed_description'),\n",
       "   (733, 742, 'Sign_symptom'),\n",
       "   (929, 937, 'Therapeutic_procedure')]})"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ner[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the Spacy NER model\n",
    "Frist, split in train - test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(df_ner)\n",
    "\n",
    "train_data = df_ner[:int(len(df_ner)*0.8)]\n",
    "test_data = df_ner[int(len(df_ner)*0.8):]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we create the binary dataset as spacy required\n",
    "to generate train file:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 982/982 [00:11<00:00, 86.22it/s] \n"
     ]
    }
   ],
   "source": [
    "db = DocBin() # create a DocBin object\n",
    "\n",
    "nlp = spacy.blank('fr') \n",
    "\n",
    "for text, annot in tqdm(test_data): # data in previous format\n",
    "    doc = nlp.make_doc(text) # create doc object from text\n",
    "    ents = []\n",
    "    for start, end, label in annot[\"entities\"]: # add character indexes\n",
    "        span = doc.char_span(start, end, label=label, alignment_mode=\"contract\")\n",
    "        if span is None:\n",
    "            print(\"Skipping entity\")\n",
    "        else:\n",
    "            ents.append(span)\n",
    "    doc.ents = ents # label the text with the ents\n",
    "    db.add(doc)\n",
    "\n",
    "db.to_disk(\"./test.spacy\") # save the docbin object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to generate test file:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 982/982 [00:09<00:00, 107.57it/s]\n"
     ]
    }
   ],
   "source": [
    "db = DocBin() # create a DocBin object\n",
    "\n",
    "nlp = spacy.blank('es') \n",
    "\n",
    "for text, annot in tqdm(test_data): # data in previous format\n",
    "    doc = nlp.make_doc(text) # create doc object from text\n",
    "    ents = []\n",
    "    for start, end, label in annot[\"entities\"]: # add character indexes\n",
    "        span = doc.char_span(start, end, label=label, alignment_mode=\"contract\")\n",
    "        if span is None:\n",
    "            print(\"Skipping entity\")\n",
    "        else:\n",
    "            ents.append(span)\n",
    "    doc.ents = ents # label the text with the ents\n",
    "    db.add(doc)\n",
    "\n",
    "db.to_disk(\"./test.spacy\") # save the docbin object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go to spacy and generate the case config file, as we show in the image below:\n",
    "download that base_config file and place in the same diretory\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Usage: python -m spacy init fill-config [OPTIONS] BASE_PATH [OUTPUT_FILE]\n",
      "Try 'python -m spacy init fill-config --help' for help.\n",
      "\n",
      "Error: Invalid value for 'BASE_PATH': File 'base_config-cpu.cfg' does not exist.\n"
     ]
    }
   ],
   "source": [
    "#!python -m spacy init fill-config base_config.cfg config.cfg\n",
    "!python -m spacy init fill-config base_config-cpu.cfg config.cfg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "✔ Auto-filled config with all values\n",
    "✔ Saved config\n",
    "config.cfg\n",
    "You can now add your data and train your pipeline:\n",
    "python -m spacy train config.cfg --paths.train ./train.spacy --paths.dev ./dev.spacy\n",
    "\n",
    "Finally Train the NER model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting https://github.com/explosion/spacy-models/releases/download/fr_core_news_lg-3.0.0/fr_core_news_lg-3.0.0.tar.gz\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/fr_core_news_lg-3.0.0/fr_core_news_lg-3.0.0.tar.gz (573.3 MB)\n",
      "     ---------------------------------------- 0.0/573.3 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/573.3 MB ? eta -:--:--\n",
      "     -------------------------------------- 0.0/573.3 MB 217.9 kB/s eta 0:43:52\n",
      "     -------------------------------------- 0.0/573.3 MB 245.8 kB/s eta 0:38:53\n",
      "     -------------------------------------- 0.1/573.3 MB 476.3 kB/s eta 0:20:04\n",
      "     ---------------------------------------- 1.4/573.3 MB 6.3 MB/s eta 0:01:31\n",
      "     --------------------------------------- 4.8/573.3 MB 17.9 MB/s eta 0:00:32\n",
      "      -------------------------------------- 8.7/573.3 MB 27.7 MB/s eta 0:00:21\n",
      "      ------------------------------------- 12.1/573.3 MB 81.8 MB/s eta 0:00:07\n",
      "     - ------------------------------------ 15.9/573.3 MB 72.6 MB/s eta 0:00:08\n",
      "     - ------------------------------------ 19.5/573.3 MB 81.8 MB/s eta 0:00:07\n",
      "     - ------------------------------------ 23.5/573.3 MB 93.0 MB/s eta 0:00:06\n",
      "     - ------------------------------------ 26.1/573.3 MB 72.6 MB/s eta 0:00:08\n",
      "     - ------------------------------------ 29.6/573.3 MB 65.6 MB/s eta 0:00:09\n",
      "     -- ----------------------------------- 33.6/573.3 MB 73.1 MB/s eta 0:00:08\n",
      "     -- ----------------------------------- 37.9/573.3 MB 81.8 MB/s eta 0:00:07\n",
      "     -- ----------------------------------- 41.3/573.3 MB 81.8 MB/s eta 0:00:07\n",
      "     -- ----------------------------------- 42.6/573.3 MB 59.5 MB/s eta 0:00:09\n",
      "     --- ---------------------------------- 46.3/573.3 MB 65.6 MB/s eta 0:00:09\n",
      "     --- ---------------------------------- 48.6/573.3 MB 54.7 MB/s eta 0:00:10\n",
      "     --- ---------------------------------- 50.7/573.3 MB 54.4 MB/s eta 0:00:10\n",
      "     --- ---------------------------------- 54.0/573.3 MB 54.7 MB/s eta 0:00:10\n",
      "     --- ---------------------------------- 55.7/573.3 MB 54.7 MB/s eta 0:00:10\n",
      "     --- ---------------------------------- 59.0/573.3 MB 59.5 MB/s eta 0:00:09\n",
      "     ---- --------------------------------- 61.2/573.3 MB 59.5 MB/s eta 0:00:09\n",
      "     ---- --------------------------------- 63.2/573.3 MB 54.7 MB/s eta 0:00:10\n",
      "     ---- --------------------------------- 66.7/573.3 MB 59.5 MB/s eta 0:00:09\n",
      "     ---- --------------------------------- 68.3/573.3 MB 54.4 MB/s eta 0:00:10\n",
      "     ---- --------------------------------- 71.6/573.3 MB 59.5 MB/s eta 0:00:09\n",
      "     ---- --------------------------------- 73.3/573.3 MB 50.1 MB/s eta 0:00:10\n",
      "     ----- -------------------------------- 75.9/573.3 MB 50.4 MB/s eta 0:00:10\n",
      "     ----- -------------------------------- 79.2/573.3 MB 59.5 MB/s eta 0:00:09\n",
      "     ----- -------------------------------- 81.0/573.3 MB 50.4 MB/s eta 0:00:10\n",
      "     ----- -------------------------------- 84.3/573.3 MB 59.5 MB/s eta 0:00:09\n",
      "     ----- -------------------------------- 86.0/573.3 MB 50.4 MB/s eta 0:00:10\n",
      "     ----- -------------------------------- 88.6/573.3 MB 54.4 MB/s eta 0:00:09\n",
      "     ------ ------------------------------- 91.8/573.3 MB 59.5 MB/s eta 0:00:09\n",
      "     ------ ------------------------------- 93.6/573.3 MB 50.1 MB/s eta 0:00:10\n",
      "     ------ ------------------------------- 97.0/573.3 MB 59.8 MB/s eta 0:00:08\n",
      "     ------ ------------------------------- 99.2/573.3 MB 59.5 MB/s eta 0:00:08\n",
      "     ------ ------------------------------ 101.5/573.3 MB 54.7 MB/s eta 0:00:09\n",
      "     ------ ------------------------------ 104.3/573.3 MB 59.5 MB/s eta 0:00:08\n",
      "     ------ ------------------------------ 106.4/573.3 MB 50.4 MB/s eta 0:00:10\n",
      "     ------- ----------------------------- 108.7/573.3 MB 54.4 MB/s eta 0:00:09\n",
      "     ------- ----------------------------- 111.7/573.3 MB 59.5 MB/s eta 0:00:08\n",
      "     ------- ----------------------------- 114.1/573.3 MB 54.4 MB/s eta 0:00:09\n",
      "     ------- ----------------------------- 117.2/573.3 MB 65.6 MB/s eta 0:00:07\n",
      "     ------- ----------------------------- 119.1/573.3 MB 59.5 MB/s eta 0:00:08\n",
      "     ------- ----------------------------- 122.5/573.3 MB 59.5 MB/s eta 0:00:08\n",
      "     -------- ---------------------------- 124.6/573.3 MB 59.5 MB/s eta 0:00:08\n",
      "     -------- ---------------------------- 126.8/573.3 MB 54.4 MB/s eta 0:00:09\n",
      "     -------- ---------------------------- 129.8/573.3 MB 59.5 MB/s eta 0:00:08\n",
      "     -------- ---------------------------- 132.2/573.3 MB 54.4 MB/s eta 0:00:09\n",
      "     -------- ---------------------------- 135.2/573.3 MB 59.5 MB/s eta 0:00:08\n",
      "     -------- ---------------------------- 137.4/573.3 MB 59.8 MB/s eta 0:00:08\n",
      "     --------- --------------------------- 139.5/573.3 MB 54.7 MB/s eta 0:00:08\n",
      "     --------- --------------------------- 142.6/573.3 MB 59.5 MB/s eta 0:00:08\n",
      "     --------- --------------------------- 144.7/573.3 MB 54.7 MB/s eta 0:00:08\n",
      "     --------- --------------------------- 147.9/573.3 MB 59.5 MB/s eta 0:00:08\n",
      "     --------- --------------------------- 149.9/573.3 MB 59.5 MB/s eta 0:00:08\n",
      "     --------- --------------------------- 152.2/573.3 MB 54.4 MB/s eta 0:00:08\n",
      "     ---------- -------------------------- 155.4/573.3 MB 59.5 MB/s eta 0:00:08\n",
      "     ---------- -------------------------- 157.2/573.3 MB 50.4 MB/s eta 0:00:09\n",
      "     ---------- -------------------------- 160.7/573.3 MB 59.5 MB/s eta 0:00:07\n",
      "     ---------- -------------------------- 162.1/573.3 MB 46.7 MB/s eta 0:00:09\n",
      "     ---------- -------------------------- 165.0/573.3 MB 54.7 MB/s eta 0:00:08\n",
      "     ---------- -------------------------- 167.6/573.3 MB 59.5 MB/s eta 0:00:07\n",
      "     ---------- -------------------------- 169.8/573.3 MB 50.4 MB/s eta 0:00:09\n",
      "     ----------- ------------------------- 173.1/573.3 MB 65.6 MB/s eta 0:00:07\n",
      "     ----------- ------------------------- 175.0/573.3 MB 54.4 MB/s eta 0:00:08\n",
      "     ----------- ------------------------- 177.8/573.3 MB 54.4 MB/s eta 0:00:08\n",
      "     ----------- ------------------------- 179.2/573.3 MB 46.9 MB/s eta 0:00:09\n",
      "     ----------- ------------------------- 182.2/573.3 MB 50.1 MB/s eta 0:00:08\n",
      "     ----------- ------------------------- 185.5/573.3 MB 54.7 MB/s eta 0:00:08\n",
      "     ------------ ------------------------ 187.9/573.3 MB 54.7 MB/s eta 0:00:08\n",
      "     ------------ ------------------------ 190.5/573.3 MB 65.6 MB/s eta 0:00:06\n",
      "     ------------ ------------------------ 193.1/573.3 MB 59.5 MB/s eta 0:00:07\n",
      "     ------------ ------------------------ 195.1/573.3 MB 54.4 MB/s eta 0:00:07\n",
      "     ------------ ------------------------ 198.6/573.3 MB 59.5 MB/s eta 0:00:07\n",
      "     ------------ ------------------------ 200.4/573.3 MB 50.4 MB/s eta 0:00:08\n",
      "     ------------- ----------------------- 203.3/573.3 MB 54.4 MB/s eta 0:00:07\n",
      "     ------------- ----------------------- 205.9/573.3 MB 59.8 MB/s eta 0:00:07\n",
      "     ------------- ----------------------- 207.9/573.3 MB 54.7 MB/s eta 0:00:07\n",
      "     ------------- ----------------------- 211.8/573.3 MB 65.6 MB/s eta 0:00:06\n",
      "     ------------- ----------------------- 213.0/573.3 MB 50.4 MB/s eta 0:00:08\n",
      "     ------------- ----------------------- 216.1/573.3 MB 54.4 MB/s eta 0:00:07\n",
      "     -------------- ---------------------- 218.5/573.3 MB 59.5 MB/s eta 0:00:06\n",
      "     -------------- ---------------------- 220.7/573.3 MB 50.4 MB/s eta 0:00:07\n",
      "     -------------- ---------------------- 224.0/573.3 MB 59.5 MB/s eta 0:00:06\n",
      "     -------------- ---------------------- 226.3/573.3 MB 54.7 MB/s eta 0:00:07\n",
      "     -------------- ---------------------- 228.8/573.3 MB 59.5 MB/s eta 0:00:06\n",
      "     -------------- ---------------------- 230.8/573.3 MB 54.7 MB/s eta 0:00:07\n",
      "     --------------- --------------------- 233.3/573.3 MB 54.7 MB/s eta 0:00:07\n",
      "     --------------- --------------------- 237.1/573.3 MB 59.5 MB/s eta 0:00:06\n",
      "     --------------- --------------------- 239.1/573.3 MB 54.4 MB/s eta 0:00:07\n",
      "     --------------- --------------------- 241.5/573.3 MB 59.5 MB/s eta 0:00:06\n",
      "     --------------- --------------------- 244.4/573.3 MB 59.5 MB/s eta 0:00:06\n",
      "     --------------- --------------------- 246.1/573.3 MB 50.4 MB/s eta 0:00:07\n",
      "     ---------------- -------------------- 249.6/573.3 MB 59.5 MB/s eta 0:00:06\n",
      "     ---------------- -------------------- 251.7/573.3 MB 54.7 MB/s eta 0:00:06\n",
      "     ---------------- -------------------- 254.2/573.3 MB 54.7 MB/s eta 0:00:06\n",
      "     ---------------- -------------------- 256.6/573.3 MB 59.5 MB/s eta 0:00:06\n",
      "     ---------------- -------------------- 258.8/573.3 MB 54.4 MB/s eta 0:00:06\n",
      "     ---------------- -------------------- 262.5/573.3 MB 59.5 MB/s eta 0:00:06\n",
      "     ----------------- ------------------- 264.1/573.3 MB 50.1 MB/s eta 0:00:07\n",
      "     ----------------- ------------------- 266.9/573.3 MB 59.8 MB/s eta 0:00:06\n",
      "     ----------------- ------------------- 269.6/573.3 MB 59.5 MB/s eta 0:00:06\n",
      "     ----------------- ------------------- 271.6/573.3 MB 50.4 MB/s eta 0:00:06\n",
      "     ----------------- ------------------- 274.3/573.3 MB 59.8 MB/s eta 0:00:05\n",
      "     ----------------- ------------------- 276.5/573.3 MB 50.4 MB/s eta 0:00:06\n",
      "     ------------------ ------------------ 279.6/573.3 MB 54.4 MB/s eta 0:00:06\n",
      "     ------------------ ------------------ 281.6/573.3 MB 54.4 MB/s eta 0:00:06\n",
      "     ------------------ ------------------ 284.2/573.3 MB 50.1 MB/s eta 0:00:06\n",
      "     ------------------ ------------------ 286.7/573.3 MB 54.7 MB/s eta 0:00:06\n",
      "     ------------------ ------------------ 289.4/573.3 MB 54.7 MB/s eta 0:00:06\n",
      "     ------------------ ------------------ 292.2/573.3 MB 54.7 MB/s eta 0:00:06\n",
      "     ------------------- ----------------- 294.8/573.3 MB 54.7 MB/s eta 0:00:06\n",
      "     ------------------- ----------------- 297.0/573.3 MB 54.4 MB/s eta 0:00:06\n",
      "     ------------------- ----------------- 299.7/573.3 MB 54.4 MB/s eta 0:00:06\n",
      "     ------------------- ----------------- 302.3/573.3 MB 54.4 MB/s eta 0:00:05\n",
      "     ------------------- ----------------- 305.0/573.3 MB 54.4 MB/s eta 0:00:05\n",
      "     ------------------- ----------------- 307.5/573.3 MB 54.7 MB/s eta 0:00:05\n",
      "     ------------------- ----------------- 309.8/573.3 MB 54.7 MB/s eta 0:00:05\n",
      "     -------------------- ---------------- 312.7/573.3 MB 54.7 MB/s eta 0:00:05\n",
      "     -------------------- ---------------- 315.2/573.3 MB 54.7 MB/s eta 0:00:05\n",
      "     -------------------- ---------------- 317.9/573.3 MB 54.4 MB/s eta 0:00:05\n",
      "     -------------------- ---------------- 320.4/573.3 MB 59.5 MB/s eta 0:00:05\n",
      "     -------------------- ---------------- 322.8/573.3 MB 54.4 MB/s eta 0:00:05\n",
      "     --------------------- --------------- 325.9/573.3 MB 59.5 MB/s eta 0:00:05\n",
      "     --------------------- --------------- 328.3/573.3 MB 54.7 MB/s eta 0:00:05\n",
      "     --------------------- --------------- 330.9/573.3 MB 54.7 MB/s eta 0:00:05\n",
      "     --------------------- --------------- 333.6/573.3 MB 59.5 MB/s eta 0:00:05\n",
      "     --------------------- --------------- 335.9/573.3 MB 54.7 MB/s eta 0:00:05\n",
      "     --------------------- --------------- 338.8/573.3 MB 59.5 MB/s eta 0:00:04\n",
      "     ---------------------- -------------- 341.3/573.3 MB 59.5 MB/s eta 0:00:04\n",
      "     ---------------------- -------------- 343.8/573.3 MB 54.4 MB/s eta 0:00:05\n",
      "     ---------------------- -------------- 346.8/573.3 MB 59.5 MB/s eta 0:00:04\n",
      "     ---------------------- -------------- 349.0/573.3 MB 54.7 MB/s eta 0:00:05\n",
      "     ---------------------- -------------- 351.9/573.3 MB 59.5 MB/s eta 0:00:04\n",
      "     ---------------------- -------------- 354.4/573.3 MB 59.5 MB/s eta 0:00:04\n",
      "     ----------------------- ------------- 356.7/573.3 MB 54.7 MB/s eta 0:00:04\n",
      "     ----------------------- ------------- 359.7/573.3 MB 59.5 MB/s eta 0:00:04\n",
      "     ----------------------- ------------- 361.9/573.3 MB 54.4 MB/s eta 0:00:04\n",
      "     ----------------------- ------------- 364.7/573.3 MB 59.5 MB/s eta 0:00:04\n",
      "     ----------------------- ------------- 367.1/573.3 MB 59.5 MB/s eta 0:00:04\n",
      "     ----------------------- ------------- 369.5/573.3 MB 54.7 MB/s eta 0:00:04\n",
      "     ------------------------ ------------ 372.7/573.3 MB 59.5 MB/s eta 0:00:04\n",
      "     ------------------------ ------------ 374.8/573.3 MB 54.7 MB/s eta 0:00:04\n",
      "     ------------------------ ------------ 377.6/573.3 MB 59.5 MB/s eta 0:00:04\n",
      "     ------------------------ ------------ 380.2/573.3 MB 59.5 MB/s eta 0:00:04\n",
      "     ------------------------ ------------ 382.4/573.3 MB 54.4 MB/s eta 0:00:04\n",
      "     ------------------------ ------------ 384.9/573.3 MB 54.4 MB/s eta 0:00:04\n",
      "     ------------------------- ----------- 387.8/573.3 MB 54.4 MB/s eta 0:00:04\n",
      "     ------------------------- ----------- 390.5/573.3 MB 54.7 MB/s eta 0:00:04\n",
      "     ------------------------- ----------- 392.9/573.3 MB 59.5 MB/s eta 0:00:04\n",
      "     ------------------------- ----------- 395.3/573.3 MB 54.7 MB/s eta 0:00:04\n",
      "     ------------------------- ----------- 398.6/573.3 MB 59.5 MB/s eta 0:00:03\n",
      "     ------------------------- ----------- 400.6/573.3 MB 54.4 MB/s eta 0:00:04\n",
      "     -------------------------- ---------- 403.3/573.3 MB 54.4 MB/s eta 0:00:04\n",
      "     -------------------------- ---------- 405.8/573.3 MB 59.5 MB/s eta 0:00:03\n",
      "     -------------------------- ---------- 408.2/573.3 MB 54.4 MB/s eta 0:00:04\n",
      "     -------------------------- ---------- 411.6/573.3 MB 59.8 MB/s eta 0:00:03\n",
      "     -------------------------- ---------- 413.7/573.3 MB 54.7 MB/s eta 0:00:03\n",
      "     -------------------------- ---------- 416.3/573.3 MB 54.7 MB/s eta 0:00:03\n",
      "     --------------------------- --------- 419.0/573.3 MB 59.5 MB/s eta 0:00:03\n",
      "     --------------------------- --------- 421.1/573.3 MB 50.4 MB/s eta 0:00:04\n",
      "     --------------------------- --------- 424.2/573.3 MB 54.4 MB/s eta 0:00:03\n",
      "     --------------------------- --------- 426.5/573.3 MB 54.4 MB/s eta 0:00:03\n",
      "     --------------------------- --------- 429.2/573.3 MB 54.4 MB/s eta 0:00:03\n",
      "     --------------------------- --------- 431.0/573.3 MB 50.4 MB/s eta 0:00:03\n",
      "     --------------------------- --------- 433.8/573.3 MB 50.4 MB/s eta 0:00:03\n",
      "     ---------------------------- -------- 437.0/573.3 MB 54.7 MB/s eta 0:00:03\n",
      "     ---------------------------- -------- 439.2/573.3 MB 54.7 MB/s eta 0:00:03\n",
      "     ---------------------------- -------- 442.1/573.3 MB 59.5 MB/s eta 0:00:03\n",
      "     ---------------------------- -------- 444.8/573.3 MB 59.5 MB/s eta 0:00:03\n",
      "     ---------------------------- -------- 445.3/573.3 MB 59.5 MB/s eta 0:00:03\n",
      "     ---------------------------- -------- 448.2/573.3 MB 46.7 MB/s eta 0:00:03\n",
      "     ----------------------------- ------- 451.7/573.3 MB 50.4 MB/s eta 0:00:03\n",
      "     ----------------------------- ------- 454.6/573.3 MB 54.7 MB/s eta 0:00:03\n",
      "     ----------------------------- ------- 457.1/573.3 MB 65.6 MB/s eta 0:00:02\n",
      "     ----------------------------- ------- 459.6/573.3 MB 59.5 MB/s eta 0:00:02\n",
      "     ----------------------------- ------- 462.5/573.3 MB 54.4 MB/s eta 0:00:03\n",
      "     ----------------------------- ------- 464.8/573.3 MB 54.4 MB/s eta 0:00:02\n",
      "     ------------------------------ ------ 467.7/573.3 MB 54.4 MB/s eta 0:00:02\n",
      "     ------------------------------ ------ 470.4/573.3 MB 54.4 MB/s eta 0:00:02\n",
      "     ------------------------------ ------ 472.6/573.3 MB 54.7 MB/s eta 0:00:02\n",
      "     ------------------------------ ------ 475.6/573.3 MB 54.7 MB/s eta 0:00:02\n",
      "     ------------------------------ ------ 478.2/573.3 MB 54.7 MB/s eta 0:00:02\n",
      "     ------------------------------- ----- 480.8/573.3 MB 54.7 MB/s eta 0:00:02\n",
      "     ------------------------------- ----- 483.5/573.3 MB 59.5 MB/s eta 0:00:02\n",
      "     ------------------------------- ----- 486.3/573.3 MB 59.5 MB/s eta 0:00:02\n",
      "     ------------------------------- ----- 489.1/573.3 MB 59.5 MB/s eta 0:00:02\n",
      "     ------------------------------- ----- 491.5/573.3 MB 59.5 MB/s eta 0:00:02\n",
      "     ------------------------------- ----- 493.7/573.3 MB 54.7 MB/s eta 0:00:02\n",
      "     -------------------------------- ---- 496.6/573.3 MB 54.7 MB/s eta 0:00:02\n",
      "     -------------------------------- ---- 498.9/573.3 MB 54.7 MB/s eta 0:00:02\n",
      "     -------------------------------- ---- 501.7/573.3 MB 54.7 MB/s eta 0:00:02\n",
      "     -------------------------------- ---- 504.1/573.3 MB 54.4 MB/s eta 0:00:02\n",
      "     -------------------------------- ---- 505.9/573.3 MB 54.7 MB/s eta 0:00:02\n",
      "     -------------------------------- ---- 509.0/573.3 MB 54.4 MB/s eta 0:00:02\n",
      "     --------------------------------- --- 511.8/573.3 MB 54.4 MB/s eta 0:00:02\n",
      "     --------------------------------- --- 514.6/573.3 MB 54.7 MB/s eta 0:00:02\n",
      "     --------------------------------- --- 517.2/573.3 MB 65.6 MB/s eta 0:00:01\n",
      "     --------------------------------- --- 519.8/573.3 MB 54.7 MB/s eta 0:00:01\n",
      "     --------------------------------- --- 522.2/573.3 MB 54.7 MB/s eta 0:00:01\n",
      "     --------------------------------- --- 524.7/573.3 MB 54.4 MB/s eta 0:00:01\n",
      "     ---------------------------------- -- 527.4/573.3 MB 54.4 MB/s eta 0:00:01\n",
      "     ---------------------------------- -- 530.0/573.3 MB 54.4 MB/s eta 0:00:01\n",
      "     ---------------------------------- -- 532.7/573.3 MB 54.4 MB/s eta 0:00:01\n",
      "     ---------------------------------- -- 535.2/573.3 MB 54.7 MB/s eta 0:00:01\n",
      "     ---------------------------------- -- 538.0/573.3 MB 54.7 MB/s eta 0:00:01\n",
      "     ---------------------------------- -- 540.5/573.3 MB 54.7 MB/s eta 0:00:01\n",
      "     ----------------------------------- - 543.3/573.3 MB 54.7 MB/s eta 0:00:01\n",
      "     ----------------------------------- - 546.2/573.3 MB 59.5 MB/s eta 0:00:01\n",
      "     ----------------------------------- - 548.7/573.3 MB 54.4 MB/s eta 0:00:01\n",
      "     ----------------------------------- - 550.3/573.3 MB 50.4 MB/s eta 0:00:01\n",
      "     ----------------------------------- - 553.6/573.3 MB 54.4 MB/s eta 0:00:01\n",
      "     ----------------------------------- - 556.3/573.3 MB 54.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  558.9/573.3 MB 54.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  561.9/573.3 MB 59.5 MB/s eta 0:00:01\n",
      "     ------------------------------------  564.6/573.3 MB 59.5 MB/s eta 0:00:01\n",
      "     ------------------------------------  567.1/573.3 MB 59.5 MB/s eta 0:00:01\n",
      "     ------------------------------------  569.8/573.3 MB 59.5 MB/s eta 0:00:01\n",
      "     ------------------------------------  572.3/573.3 MB 54.4 MB/s eta 0:00:01\n",
      "     ------------------------------------  573.3/573.3 MB 54.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  573.3/573.3 MB 54.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  573.3/573.3 MB 54.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  573.3/573.3 MB 54.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  573.3/573.3 MB 54.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  573.3/573.3 MB 54.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  573.3/573.3 MB 54.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  573.3/573.3 MB 54.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  573.3/573.3 MB 54.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  573.3/573.3 MB 54.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  573.3/573.3 MB 54.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  573.3/573.3 MB 54.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  573.3/573.3 MB 54.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  573.3/573.3 MB 54.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  573.3/573.3 MB 54.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  573.3/573.3 MB 54.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  573.3/573.3 MB 54.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  573.3/573.3 MB 54.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  573.3/573.3 MB 54.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  573.3/573.3 MB 54.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  573.3/573.3 MB 54.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  573.3/573.3 MB 54.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  573.3/573.3 MB 54.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  573.3/573.3 MB 54.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  573.3/573.3 MB 54.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  573.3/573.3 MB 54.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  573.3/573.3 MB 54.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  573.3/573.3 MB 54.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  573.3/573.3 MB 54.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  573.3/573.3 MB 54.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  573.3/573.3 MB 54.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  573.3/573.3 MB 54.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  573.3/573.3 MB 54.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  573.3/573.3 MB 54.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  573.3/573.3 MB 54.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  573.3/573.3 MB 54.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  573.3/573.3 MB 54.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  573.3/573.3 MB 54.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  573.3/573.3 MB 54.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  573.3/573.3 MB 54.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  573.3/573.3 MB 54.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  573.3/573.3 MB 54.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  573.3/573.3 MB 54.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  573.3/573.3 MB 54.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  573.3/573.3 MB 54.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  573.3/573.3 MB 54.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  573.3/573.3 MB 54.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  573.3/573.3 MB 54.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  573.3/573.3 MB 54.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  573.3/573.3 MB 54.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  573.3/573.3 MB 54.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  573.3/573.3 MB 54.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  573.3/573.3 MB 54.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  573.3/573.3 MB 54.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  573.3/573.3 MB 54.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  573.3/573.3 MB 54.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  573.3/573.3 MB 54.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  573.3/573.3 MB 54.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  573.3/573.3 MB 54.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  573.3/573.3 MB 54.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  573.3/573.3 MB 54.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  573.3/573.3 MB 54.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  573.3/573.3 MB 54.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  573.3/573.3 MB 54.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  573.3/573.3 MB 54.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  573.3/573.3 MB 54.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  573.3/573.3 MB 54.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  573.3/573.3 MB 54.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  573.3/573.3 MB 54.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  573.3/573.3 MB 54.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  573.3/573.3 MB 54.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  573.3/573.3 MB 54.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  573.3/573.3 MB 54.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  573.3/573.3 MB 54.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  573.3/573.3 MB 54.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  573.3/573.3 MB 54.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  573.3/573.3 MB 54.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  573.3/573.3 MB 54.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  573.3/573.3 MB 54.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  573.3/573.3 MB 54.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  573.3/573.3 MB 54.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  573.3/573.3 MB 54.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  573.3/573.3 MB 54.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  573.3/573.3 MB 54.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  573.3/573.3 MB 54.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  573.3/573.3 MB 54.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  573.3/573.3 MB 54.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  573.3/573.3 MB 54.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  573.3/573.3 MB 54.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  573.3/573.3 MB 54.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  573.3/573.3 MB 54.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  573.3/573.3 MB 54.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  573.3/573.3 MB 54.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  573.3/573.3 MB 54.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  573.3/573.3 MB 54.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  573.3/573.3 MB 54.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  573.3/573.3 MB 54.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  573.3/573.3 MB 54.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  573.3/573.3 MB 54.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  573.3/573.3 MB 54.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  573.3/573.3 MB 54.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  573.3/573.3 MB 54.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  573.3/573.3 MB 54.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  573.3/573.3 MB 54.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  573.3/573.3 MB 54.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  573.3/573.3 MB 54.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  573.3/573.3 MB 54.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  573.3/573.3 MB 54.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  573.3/573.3 MB 54.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  573.3/573.3 MB 54.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  573.3/573.3 MB 54.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  573.3/573.3 MB 54.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  573.3/573.3 MB 54.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  573.3/573.3 MB 54.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  573.3/573.3 MB 54.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  573.3/573.3 MB 54.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  573.3/573.3 MB 54.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  573.3/573.3 MB 54.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  573.3/573.3 MB 54.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  573.3/573.3 MB 54.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  573.3/573.3 MB 54.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  573.3/573.3 MB 54.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  573.3/573.3 MB 54.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  573.3/573.3 MB 54.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  573.3/573.3 MB 54.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  573.3/573.3 MB 54.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  573.3/573.3 MB 54.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  573.3/573.3 MB 54.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  573.3/573.3 MB 54.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  573.3/573.3 MB 54.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  573.3/573.3 MB 54.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  573.3/573.3 MB 54.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  573.3/573.3 MB 54.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  573.3/573.3 MB 54.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  573.3/573.3 MB 54.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  573.3/573.3 MB 54.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  573.3/573.3 MB 54.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  573.3/573.3 MB 54.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  573.3/573.3 MB 54.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  573.3/573.3 MB 54.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  573.3/573.3 MB 54.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  573.3/573.3 MB 54.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  573.3/573.3 MB 54.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  573.3/573.3 MB 54.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  573.3/573.3 MB 54.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  573.3/573.3 MB 54.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  573.3/573.3 MB 54.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  573.3/573.3 MB 54.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  573.3/573.3 MB 54.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  573.3/573.3 MB 54.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  573.3/573.3 MB 54.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  573.3/573.3 MB 54.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  573.3/573.3 MB 54.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  573.3/573.3 MB 54.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  573.3/573.3 MB 54.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  573.3/573.3 MB 54.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  573.3/573.3 MB 54.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  573.3/573.3 MB 54.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  573.3/573.3 MB 54.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  573.3/573.3 MB 54.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  573.3/573.3 MB 54.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  573.3/573.3 MB 54.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  573.3/573.3 MB 54.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  573.3/573.3 MB 54.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  573.3/573.3 MB 54.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  573.3/573.3 MB 54.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  573.3/573.3 MB 54.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  573.3/573.3 MB 54.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  573.3/573.3 MB 54.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  573.3/573.3 MB 54.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  573.3/573.3 MB 54.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  573.3/573.3 MB 54.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  573.3/573.3 MB 54.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  573.3/573.3 MB 54.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  573.3/573.3 MB 54.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  573.3/573.3 MB 54.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  573.3/573.3 MB 54.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  573.3/573.3 MB 54.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  573.3/573.3 MB 54.7 MB/s eta 0:00:01\n",
      "     -------------------------------------- 573.3/573.3 MB 1.1 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting spacy<3.1.0,>=3.0.0 (from fr-core-news-lg==3.0.0)\n",
      "  Downloading spacy-3.0.9.tar.gz (989 kB)\n",
      "     ---------------------------------------- 0.0/989.2 kB ? eta -:--:--\n",
      "     ---------------------------------------- 10.2/989.2 kB ? eta -:--:--\n",
      "     ---------------------------------------- 10.2/989.2 kB ? eta -:--:--\n",
      "     - ----------------------------------- 41.0/989.2 kB 281.8 kB/s eta 0:00:04\n",
      "     ------- ------------------------------ 204.8/989.2 kB 1.2 MB/s eta 0:00:01\n",
      "     -------------------------------------  983.0/989.2 kB 4.8 MB/s eta 0:00:01\n",
      "     -------------------------------------  983.0/989.2 kB 4.8 MB/s eta 0:00:01\n",
      "     -------------------------------------  983.0/989.2 kB 4.8 MB/s eta 0:00:01\n",
      "     -------------------------------------  983.0/989.2 kB 4.8 MB/s eta 0:00:01\n",
      "     -------------------------------------  983.0/989.2 kB 4.8 MB/s eta 0:00:01\n",
      "     -------------------------------------  983.0/989.2 kB 4.8 MB/s eta 0:00:01\n",
      "     -------------------------------------- 989.2/989.2 kB 2.2 MB/s eta 0:00:00\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: still running...\n",
      "  Installing build dependencies: finished with status 'error'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  × pip subprocess to install build dependencies did not run successfully.\n",
      "  │ exit code: 1\n",
      "  ╰─> [498 lines of output]\n",
      "      Collecting setuptools\n",
      "        Using cached setuptools-69.0.2-py3-none-any.whl.metadata (6.3 kB)\n",
      "      Collecting cython<3.0,>=0.25\n",
      "        Downloading Cython-0.29.37-py2.py3-none-any.whl.metadata (3.1 kB)\n",
      "      Collecting cymem<2.1.0,>=2.0.2\n",
      "        Using cached cymem-2.0.8-cp311-cp311-win_amd64.whl.metadata (8.6 kB)\n",
      "      Collecting preshed<3.1.0,>=3.0.2\n",
      "        Using cached preshed-3.0.9-cp311-cp311-win_amd64.whl.metadata (2.2 kB)\n",
      "      Collecting murmurhash<1.1.0,>=0.28.0\n",
      "        Using cached murmurhash-1.0.10-cp311-cp311-win_amd64.whl.metadata (2.0 kB)\n",
      "      Collecting thinc<8.1.0,>=8.0.3\n",
      "        Downloading thinc-8.0.17.tar.gz (189 kB)\n",
      "           ---------------------------------------- 0.0/189.7 kB ? eta -:--:--\n",
      "           ---------------------------------------- 0.0/189.7 kB ? eta -:--:--\n",
      "           -- ------------------------------------- 10.2/189.7 kB ? eta -:--:--\n",
      "           ----------- ------------------------- 61.4/189.7 kB 812.7 kB/s eta 0:00:01\n",
      "           ------------------------------------ - 184.3/189.7 kB 2.2 MB/s eta 0:00:01\n",
      "           -------------------------------------- 189.7/189.7 kB 1.4 MB/s eta 0:00:00\n",
      "        Installing build dependencies: started\n",
      "        Installing build dependencies: finished with status 'done'\n",
      "        Getting requirements to build wheel: started\n",
      "        Getting requirements to build wheel: finished with status 'done'\n",
      "        Installing backend dependencies: started\n",
      "        Installing backend dependencies: finished with status 'done'\n",
      "        Preparing metadata (pyproject.toml): started\n",
      "        Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "      Collecting blis<0.8.0,>=0.4.0\n",
      "        Using cached blis-0.7.11-cp311-cp311-win_amd64.whl.metadata (7.6 kB)\n",
      "      Collecting pathy\n",
      "        Downloading pathy-0.10.3-py3-none-any.whl.metadata (16 kB)\n",
      "      Collecting numpy>=1.15.0\n",
      "        Using cached numpy-1.26.2-cp311-cp311-win_amd64.whl.metadata (61 kB)\n",
      "      Collecting wasabi<1.1.0,>=0.8.1 (from thinc<8.1.0,>=8.0.3)\n",
      "        Downloading wasabi-0.10.1-py3-none-any.whl (26 kB)\n",
      "      Collecting srsly<3.0.0,>=2.4.0 (from thinc<8.1.0,>=8.0.3)\n",
      "        Using cached srsly-2.4.8-cp311-cp311-win_amd64.whl.metadata (20 kB)\n",
      "      Collecting catalogue<2.1.0,>=2.0.4 (from thinc<8.1.0,>=8.0.3)\n",
      "        Using cached catalogue-2.0.10-py3-none-any.whl.metadata (14 kB)\n",
      "      Collecting pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 (from thinc<8.1.0,>=8.0.3)\n",
      "        Downloading pydantic-1.8.2-py3-none-any.whl (126 kB)\n",
      "           ---------------------------------------- 0.0/126.0 kB ? eta -:--:--\n",
      "           -------------------------------------- 126.0/126.0 kB 7.2 MB/s eta 0:00:00\n",
      "      Collecting smart-open<7.0.0,>=5.2.1 (from pathy)\n",
      "        Downloading smart_open-6.4.0-py3-none-any.whl.metadata (21 kB)\n",
      "      Collecting typer<1.0.0,>=0.3.0 (from pathy)\n",
      "        Using cached typer-0.9.0-py3-none-any.whl (45 kB)\n",
      "      Collecting typing-extensions>=3.7.4.3 (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->thinc<8.1.0,>=8.0.3)\n",
      "        Downloading typing_extensions-4.9.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "      Collecting click<9.0.0,>=7.1.1 (from typer<1.0.0,>=0.3.0->pathy)\n",
      "        Downloading click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
      "      Collecting colorama (from click<9.0.0,>=7.1.1->typer<1.0.0,>=0.3.0->pathy)\n",
      "        Using cached colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
      "      Using cached setuptools-69.0.2-py3-none-any.whl (819 kB)\n",
      "      Using cached Cython-0.29.37-py2.py3-none-any.whl (989 kB)\n",
      "      Using cached cymem-2.0.8-cp311-cp311-win_amd64.whl (39 kB)\n",
      "      Using cached preshed-3.0.9-cp311-cp311-win_amd64.whl (122 kB)\n",
      "      Using cached murmurhash-1.0.10-cp311-cp311-win_amd64.whl (25 kB)\n",
      "      Using cached blis-0.7.11-cp311-cp311-win_amd64.whl (6.6 MB)\n",
      "      Downloading pathy-0.10.3-py3-none-any.whl (48 kB)\n",
      "         ---------------------------------------- 0.0/48.9 kB ? eta -:--:--\n",
      "         ---------------------------------------- 48.9/48.9 kB 2.4 MB/s eta 0:00:00\n",
      "      Using cached numpy-1.26.2-cp311-cp311-win_amd64.whl (15.8 MB)\n",
      "      Using cached catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
      "      Downloading smart_open-6.4.0-py3-none-any.whl (57 kB)\n",
      "         ---------------------------------------- 0.0/57.0 kB ? eta -:--:--\n",
      "         ---------------------------------------- 57.0/57.0 kB 2.9 MB/s eta 0:00:00\n",
      "      Using cached srsly-2.4.8-cp311-cp311-win_amd64.whl (479 kB)\n",
      "      Downloading click-8.1.7-py3-none-any.whl (97 kB)\n",
      "         ---------------------------------------- 0.0/97.9 kB ? eta -:--:--\n",
      "         ---------------------------------------- 97.9/97.9 kB 5.8 MB/s eta 0:00:00\n",
      "      Downloading typing_extensions-4.9.0-py3-none-any.whl (32 kB)\n",
      "      Building wheels for collected packages: thinc\n",
      "        Building wheel for thinc (pyproject.toml): started\n",
      "        Building wheel for thinc (pyproject.toml): finished with status 'error'\n",
      "        error: subprocess-exited-with-error\n",
      "      \n",
      "        × Building wheel for thinc (pyproject.toml) did not run successfully.\n",
      "        │ exit code: 1\n",
      "        ╰─> [413 lines of output]\n",
      "            Cythonizing sources\n",
      "            running bdist_wheel\n",
      "            running build\n",
      "            running build_py\n",
      "            creating build\n",
      "            creating build\\lib.win-amd64-cpython-311\n",
      "            creating build\\lib.win-amd64-cpython-311\\thinc\n",
      "            copying thinc\\about.py -> build\\lib.win-amd64-cpython-311\\thinc\n",
      "            copying thinc\\api.py -> build\\lib.win-amd64-cpython-311\\thinc\n",
      "            copying thinc\\config.py -> build\\lib.win-amd64-cpython-311\\thinc\n",
      "            copying thinc\\initializers.py -> build\\lib.win-amd64-cpython-311\\thinc\n",
      "            copying thinc\\loss.py -> build\\lib.win-amd64-cpython-311\\thinc\n",
      "            copying thinc\\model.py -> build\\lib.win-amd64-cpython-311\\thinc\n",
      "            copying thinc\\mypy.py -> build\\lib.win-amd64-cpython-311\\thinc\n",
      "            copying thinc\\optimizers.py -> build\\lib.win-amd64-cpython-311\\thinc\n",
      "            copying thinc\\schedules.py -> build\\lib.win-amd64-cpython-311\\thinc\n",
      "            copying thinc\\types.py -> build\\lib.win-amd64-cpython-311\\thinc\n",
      "            copying thinc\\util.py -> build\\lib.win-amd64-cpython-311\\thinc\n",
      "            copying thinc\\__init__.py -> build\\lib.win-amd64-cpython-311\\thinc\n",
      "            creating build\\lib.win-amd64-cpython-311\\thinc\\backends\n",
      "            copying thinc\\backends\\cupy_ops.py -> build\\lib.win-amd64-cpython-311\\thinc\\backends\n",
      "            copying thinc\\backends\\ops.py -> build\\lib.win-amd64-cpython-311\\thinc\\backends\n",
      "            copying thinc\\backends\\_cupy_allocators.py -> build\\lib.win-amd64-cpython-311\\thinc\\backends\n",
      "            copying thinc\\backends\\_custom_kernels.py -> build\\lib.win-amd64-cpython-311\\thinc\\backends\n",
      "            copying thinc\\backends\\_param_server.py -> build\\lib.win-amd64-cpython-311\\thinc\\backends\n",
      "            copying thinc\\backends\\__init__.py -> build\\lib.win-amd64-cpython-311\\thinc\\backends\n",
      "            creating build\\lib.win-amd64-cpython-311\\thinc\\extra\n",
      "            copying thinc\\extra\\__init__.py -> build\\lib.win-amd64-cpython-311\\thinc\\extra\n",
      "            creating build\\lib.win-amd64-cpython-311\\thinc\\layers\n",
      "            copying thinc\\layers\\add.py -> build\\lib.win-amd64-cpython-311\\thinc\\layers\n",
      "            copying thinc\\layers\\array_getitem.py -> build\\lib.win-amd64-cpython-311\\thinc\\layers\n",
      "            copying thinc\\layers\\bidirectional.py -> build\\lib.win-amd64-cpython-311\\thinc\\layers\n",
      "            copying thinc\\layers\\cauchysimilarity.py -> build\\lib.win-amd64-cpython-311\\thinc\\layers\n",
      "            copying thinc\\layers\\chain.py -> build\\lib.win-amd64-cpython-311\\thinc\\layers\n",
      "            copying thinc\\layers\\clipped_linear.py -> build\\lib.win-amd64-cpython-311\\thinc\\layers\n",
      "            copying thinc\\layers\\clone.py -> build\\lib.win-amd64-cpython-311\\thinc\\layers\n",
      "            copying thinc\\layers\\concatenate.py -> build\\lib.win-amd64-cpython-311\\thinc\\layers\n",
      "            copying thinc\\layers\\dropout.py -> build\\lib.win-amd64-cpython-311\\thinc\\layers\n",
      "            copying thinc\\layers\\embed.py -> build\\lib.win-amd64-cpython-311\\thinc\\layers\n",
      "            copying thinc\\layers\\expand_window.py -> build\\lib.win-amd64-cpython-311\\thinc\\layers\n",
      "            copying thinc\\layers\\gelu.py -> build\\lib.win-amd64-cpython-311\\thinc\\layers\n",
      "            copying thinc\\layers\\hard_swish.py -> build\\lib.win-amd64-cpython-311\\thinc\\layers\n",
      "            copying thinc\\layers\\hard_swish_mobilenet.py -> build\\lib.win-amd64-cpython-311\\thinc\\layers\n",
      "            copying thinc\\layers\\hashembed.py -> build\\lib.win-amd64-cpython-311\\thinc\\layers\n",
      "            copying thinc\\layers\\layernorm.py -> build\\lib.win-amd64-cpython-311\\thinc\\layers\n",
      "            copying thinc\\layers\\linear.py -> build\\lib.win-amd64-cpython-311\\thinc\\layers\n",
      "            copying thinc\\layers\\list2array.py -> build\\lib.win-amd64-cpython-311\\thinc\\layers\n",
      "            copying thinc\\layers\\list2padded.py -> build\\lib.win-amd64-cpython-311\\thinc\\layers\n",
      "            copying thinc\\layers\\list2ragged.py -> build\\lib.win-amd64-cpython-311\\thinc\\layers\n",
      "            copying thinc\\layers\\logistic.py -> build\\lib.win-amd64-cpython-311\\thinc\\layers\n",
      "            copying thinc\\layers\\lstm.py -> build\\lib.win-amd64-cpython-311\\thinc\\layers\n",
      "            copying thinc\\layers\\map_list.py -> build\\lib.win-amd64-cpython-311\\thinc\\layers\n",
      "            copying thinc\\layers\\maxout.py -> build\\lib.win-amd64-cpython-311\\thinc\\layers\n",
      "            copying thinc\\layers\\mish.py -> build\\lib.win-amd64-cpython-311\\thinc\\layers\n",
      "            copying thinc\\layers\\multisoftmax.py -> build\\lib.win-amd64-cpython-311\\thinc\\layers\n",
      "            copying thinc\\layers\\mxnetwrapper.py -> build\\lib.win-amd64-cpython-311\\thinc\\layers\n",
      "            copying thinc\\layers\\noop.py -> build\\lib.win-amd64-cpython-311\\thinc\\layers\n",
      "            copying thinc\\layers\\padded2list.py -> build\\lib.win-amd64-cpython-311\\thinc\\layers\n",
      "            copying thinc\\layers\\parametricattention.py -> build\\lib.win-amd64-cpython-311\\thinc\\layers\n",
      "            copying thinc\\layers\\pytorchwrapper.py -> build\\lib.win-amd64-cpython-311\\thinc\\layers\n",
      "            copying thinc\\layers\\ragged2list.py -> build\\lib.win-amd64-cpython-311\\thinc\\layers\n",
      "            copying thinc\\layers\\reduce_first.py -> build\\lib.win-amd64-cpython-311\\thinc\\layers\n",
      "            copying thinc\\layers\\reduce_last.py -> build\\lib.win-amd64-cpython-311\\thinc\\layers\n",
      "            copying thinc\\layers\\reduce_max.py -> build\\lib.win-amd64-cpython-311\\thinc\\layers\n",
      "            copying thinc\\layers\\reduce_mean.py -> build\\lib.win-amd64-cpython-311\\thinc\\layers\n",
      "            copying thinc\\layers\\reduce_sum.py -> build\\lib.win-amd64-cpython-311\\thinc\\layers\n",
      "            copying thinc\\layers\\relu.py -> build\\lib.win-amd64-cpython-311\\thinc\\layers\n",
      "            copying thinc\\layers\\remap_ids.py -> build\\lib.win-amd64-cpython-311\\thinc\\layers\n",
      "            copying thinc\\layers\\residual.py -> build\\lib.win-amd64-cpython-311\\thinc\\layers\n",
      "            copying thinc\\layers\\resizable.py -> build\\lib.win-amd64-cpython-311\\thinc\\layers\n",
      "            copying thinc\\layers\\siamese.py -> build\\lib.win-amd64-cpython-311\\thinc\\layers\n",
      "            copying thinc\\layers\\sigmoid.py -> build\\lib.win-amd64-cpython-311\\thinc\\layers\n",
      "            copying thinc\\layers\\sigmoid_activation.py -> build\\lib.win-amd64-cpython-311\\thinc\\layers\n",
      "            copying thinc\\layers\\softmax.py -> build\\lib.win-amd64-cpython-311\\thinc\\layers\n",
      "            copying thinc\\layers\\softmax_activation.py -> build\\lib.win-amd64-cpython-311\\thinc\\layers\n",
      "            copying thinc\\layers\\strings2arrays.py -> build\\lib.win-amd64-cpython-311\\thinc\\layers\n",
      "            copying thinc\\layers\\swish.py -> build\\lib.win-amd64-cpython-311\\thinc\\layers\n",
      "            copying thinc\\layers\\tensorflowwrapper.py -> build\\lib.win-amd64-cpython-311\\thinc\\layers\n",
      "            copying thinc\\layers\\tuplify.py -> build\\lib.win-amd64-cpython-311\\thinc\\layers\n",
      "            copying thinc\\layers\\uniqued.py -> build\\lib.win-amd64-cpython-311\\thinc\\layers\n",
      "            copying thinc\\layers\\with_array.py -> build\\lib.win-amd64-cpython-311\\thinc\\layers\n",
      "            copying thinc\\layers\\with_array2d.py -> build\\lib.win-amd64-cpython-311\\thinc\\layers\n",
      "            copying thinc\\layers\\with_cpu.py -> build\\lib.win-amd64-cpython-311\\thinc\\layers\n",
      "            copying thinc\\layers\\with_debug.py -> build\\lib.win-amd64-cpython-311\\thinc\\layers\n",
      "            copying thinc\\layers\\with_flatten.py -> build\\lib.win-amd64-cpython-311\\thinc\\layers\n",
      "            copying thinc\\layers\\with_getitem.py -> build\\lib.win-amd64-cpython-311\\thinc\\layers\n",
      "            copying thinc\\layers\\with_list.py -> build\\lib.win-amd64-cpython-311\\thinc\\layers\n",
      "            copying thinc\\layers\\with_nvtx_range.py -> build\\lib.win-amd64-cpython-311\\thinc\\layers\n",
      "            copying thinc\\layers\\with_padded.py -> build\\lib.win-amd64-cpython-311\\thinc\\layers\n",
      "            copying thinc\\layers\\with_ragged.py -> build\\lib.win-amd64-cpython-311\\thinc\\layers\n",
      "            copying thinc\\layers\\with_reshape.py -> build\\lib.win-amd64-cpython-311\\thinc\\layers\n",
      "            copying thinc\\layers\\__init__.py -> build\\lib.win-amd64-cpython-311\\thinc\\layers\n",
      "            creating build\\lib.win-amd64-cpython-311\\thinc\\shims\n",
      "            copying thinc\\shims\\mxnet.py -> build\\lib.win-amd64-cpython-311\\thinc\\shims\n",
      "            copying thinc\\shims\\pytorch.py -> build\\lib.win-amd64-cpython-311\\thinc\\shims\n",
      "            copying thinc\\shims\\pytorch_grad_scaler.py -> build\\lib.win-amd64-cpython-311\\thinc\\shims\n",
      "            copying thinc\\shims\\shim.py -> build\\lib.win-amd64-cpython-311\\thinc\\shims\n",
      "            copying thinc\\shims\\tensorflow.py -> build\\lib.win-amd64-cpython-311\\thinc\\shims\n",
      "            copying thinc\\shims\\__init__.py -> build\\lib.win-amd64-cpython-311\\thinc\\shims\n",
      "            creating build\\lib.win-amd64-cpython-311\\thinc\\tests\n",
      "            copying thinc\\tests\\conftest.py -> build\\lib.win-amd64-cpython-311\\thinc\\tests\n",
      "            copying thinc\\tests\\strategies.py -> build\\lib.win-amd64-cpython-311\\thinc\\tests\n",
      "            copying thinc\\tests\\test_config.py -> build\\lib.win-amd64-cpython-311\\thinc\\tests\n",
      "            copying thinc\\tests\\test_examples.py -> build\\lib.win-amd64-cpython-311\\thinc\\tests\n",
      "            copying thinc\\tests\\test_indexing.py -> build\\lib.win-amd64-cpython-311\\thinc\\tests\n",
      "            copying thinc\\tests\\test_initializers.py -> build\\lib.win-amd64-cpython-311\\thinc\\tests\n",
      "            copying thinc\\tests\\test_loss.py -> build\\lib.win-amd64-cpython-311\\thinc\\tests\n",
      "            copying thinc\\tests\\test_optimizers.py -> build\\lib.win-amd64-cpython-311\\thinc\\tests\n",
      "            copying thinc\\tests\\test_schedules.py -> build\\lib.win-amd64-cpython-311\\thinc\\tests\n",
      "            copying thinc\\tests\\test_serialize.py -> build\\lib.win-amd64-cpython-311\\thinc\\tests\n",
      "            copying thinc\\tests\\test_types.py -> build\\lib.win-amd64-cpython-311\\thinc\\tests\n",
      "            copying thinc\\tests\\test_util.py -> build\\lib.win-amd64-cpython-311\\thinc\\tests\n",
      "            copying thinc\\tests\\util.py -> build\\lib.win-amd64-cpython-311\\thinc\\tests\n",
      "            copying thinc\\tests\\__init__.py -> build\\lib.win-amd64-cpython-311\\thinc\\tests\n",
      "            creating build\\lib.win-amd64-cpython-311\\thinc\\extra\\tests\n",
      "            copying thinc\\extra\\tests\\__init__.py -> build\\lib.win-amd64-cpython-311\\thinc\\extra\\tests\n",
      "            creating build\\lib.win-amd64-cpython-311\\thinc\\tests\\backends\n",
      "            copying thinc\\tests\\backends\\test_mem.py -> build\\lib.win-amd64-cpython-311\\thinc\\tests\\backends\n",
      "            copying thinc\\tests\\backends\\test_ops.py -> build\\lib.win-amd64-cpython-311\\thinc\\tests\\backends\n",
      "            copying thinc\\tests\\backends\\__init__.py -> build\\lib.win-amd64-cpython-311\\thinc\\tests\\backends\n",
      "            creating build\\lib.win-amd64-cpython-311\\thinc\\tests\\extra\n",
      "            copying thinc\\tests\\extra\\test_beam_search.py -> build\\lib.win-amd64-cpython-311\\thinc\\tests\\extra\n",
      "            copying thinc\\tests\\extra\\__init__.py -> build\\lib.win-amd64-cpython-311\\thinc\\tests\\extra\n",
      "            creating build\\lib.win-amd64-cpython-311\\thinc\\tests\\layers\n",
      "            copying thinc\\tests\\layers\\test_basic_tagger.py -> build\\lib.win-amd64-cpython-311\\thinc\\tests\\layers\n",
      "            copying thinc\\tests\\layers\\test_combinators.py -> build\\lib.win-amd64-cpython-311\\thinc\\tests\\layers\n",
      "            copying thinc\\tests\\layers\\test_feed_forward.py -> build\\lib.win-amd64-cpython-311\\thinc\\tests\\layers\n",
      "            copying thinc\\tests\\layers\\test_hash_embed.py -> build\\lib.win-amd64-cpython-311\\thinc\\tests\\layers\n",
      "            copying thinc\\tests\\layers\\test_layers_api.py -> build\\lib.win-amd64-cpython-311\\thinc\\tests\\layers\n",
      "            copying thinc\\tests\\layers\\test_linear.py -> build\\lib.win-amd64-cpython-311\\thinc\\tests\\layers\n",
      "            copying thinc\\tests\\layers\\test_lstm.py -> build\\lib.win-amd64-cpython-311\\thinc\\tests\\layers\n",
      "            copying thinc\\tests\\layers\\test_mnist.py -> build\\lib.win-amd64-cpython-311\\thinc\\tests\\layers\n",
      "            copying thinc\\tests\\layers\\test_mxnet_wrapper.py -> build\\lib.win-amd64-cpython-311\\thinc\\tests\\layers\n",
      "            copying thinc\\tests\\layers\\test_pytorch_wrapper.py -> build\\lib.win-amd64-cpython-311\\thinc\\tests\\layers\n",
      "            copying thinc\\tests\\layers\\test_reduce.py -> build\\lib.win-amd64-cpython-311\\thinc\\tests\\layers\n",
      "            copying thinc\\tests\\layers\\test_shim.py -> build\\lib.win-amd64-cpython-311\\thinc\\tests\\layers\n",
      "            copying thinc\\tests\\layers\\test_softmax.py -> build\\lib.win-amd64-cpython-311\\thinc\\tests\\layers\n",
      "            copying thinc\\tests\\layers\\test_sparse_linear.py -> build\\lib.win-amd64-cpython-311\\thinc\\tests\\layers\n",
      "            copying thinc\\tests\\layers\\test_tensorflow_wrapper.py -> build\\lib.win-amd64-cpython-311\\thinc\\tests\\layers\n",
      "            copying thinc\\tests\\layers\\test_transforms.py -> build\\lib.win-amd64-cpython-311\\thinc\\tests\\layers\n",
      "            copying thinc\\tests\\layers\\test_uniqued.py -> build\\lib.win-amd64-cpython-311\\thinc\\tests\\layers\n",
      "            copying thinc\\tests\\layers\\test_with_debug.py -> build\\lib.win-amd64-cpython-311\\thinc\\tests\\layers\n",
      "            copying thinc\\tests\\layers\\test_with_transforms.py -> build\\lib.win-amd64-cpython-311\\thinc\\tests\\layers\n",
      "            copying thinc\\tests\\layers\\__init__.py -> build\\lib.win-amd64-cpython-311\\thinc\\tests\\layers\n",
      "            creating build\\lib.win-amd64-cpython-311\\thinc\\tests\\model\n",
      "            copying thinc\\tests\\model\\test_model.py -> build\\lib.win-amd64-cpython-311\\thinc\\tests\\model\n",
      "            copying thinc\\tests\\model\\test_validation.py -> build\\lib.win-amd64-cpython-311\\thinc\\tests\\model\n",
      "            copying thinc\\tests\\model\\__init__.py -> build\\lib.win-amd64-cpython-311\\thinc\\tests\\model\n",
      "            creating build\\lib.win-amd64-cpython-311\\thinc\\tests\\mypy\n",
      "            copying thinc\\tests\\mypy\\test_mypy.py -> build\\lib.win-amd64-cpython-311\\thinc\\tests\\mypy\n",
      "            copying thinc\\tests\\mypy\\__init__.py -> build\\lib.win-amd64-cpython-311\\thinc\\tests\\mypy\n",
      "            creating build\\lib.win-amd64-cpython-311\\thinc\\tests\\regression\n",
      "            copying thinc\\tests\\regression\\test_issue208.py -> build\\lib.win-amd64-cpython-311\\thinc\\tests\\regression\n",
      "            copying thinc\\tests\\regression\\test_issue564.py -> build\\lib.win-amd64-cpython-311\\thinc\\tests\\regression\n",
      "            copying thinc\\tests\\regression\\__init__.py -> build\\lib.win-amd64-cpython-311\\thinc\\tests\\regression\n",
      "            creating build\\lib.win-amd64-cpython-311\\thinc\\tests\\shims\n",
      "            copying thinc\\tests\\shims\\test_pytorch_grad_scaler.py -> build\\lib.win-amd64-cpython-311\\thinc\\tests\\shims\n",
      "            copying thinc\\tests\\shims\\__init__.py -> build\\lib.win-amd64-cpython-311\\thinc\\tests\\shims\n",
      "            creating build\\lib.win-amd64-cpython-311\\thinc\\tests\\mypy\\modules\n",
      "            copying thinc\\tests\\mypy\\modules\\fail_no_plugin.py -> build\\lib.win-amd64-cpython-311\\thinc\\tests\\mypy\\modules\n",
      "            copying thinc\\tests\\mypy\\modules\\fail_plugin.py -> build\\lib.win-amd64-cpython-311\\thinc\\tests\\mypy\\modules\n",
      "            copying thinc\\tests\\mypy\\modules\\success_no_plugin.py -> build\\lib.win-amd64-cpython-311\\thinc\\tests\\mypy\\modules\n",
      "            copying thinc\\tests\\mypy\\modules\\success_plugin.py -> build\\lib.win-amd64-cpython-311\\thinc\\tests\\mypy\\modules\n",
      "            copying thinc\\tests\\mypy\\modules\\__init__.py -> build\\lib.win-amd64-cpython-311\\thinc\\tests\\mypy\\modules\n",
      "            creating build\\lib.win-amd64-cpython-311\\thinc\\tests\\regression\\issue519\n",
      "            copying thinc\\tests\\regression\\issue519\\program.py -> build\\lib.win-amd64-cpython-311\\thinc\\tests\\regression\\issue519\n",
      "            copying thinc\\tests\\regression\\issue519\\test_issue519.py -> build\\lib.win-amd64-cpython-311\\thinc\\tests\\regression\\issue519\n",
      "            copying thinc\\tests\\regression\\issue519\\__init__.py -> build\\lib.win-amd64-cpython-311\\thinc\\tests\\regression\\issue519\n",
      "            running egg_info\n",
      "            writing thinc.egg-info\\PKG-INFO\n",
      "            writing dependency_links to thinc.egg-info\\dependency_links.txt\n",
      "            writing requirements to thinc.egg-info\\requires.txt\n",
      "            writing top-level names to thinc.egg-info\\top_level.txt\n",
      "            reading manifest file 'thinc.egg-info\\SOURCES.txt'\n",
      "            reading manifest template 'MANIFEST.in'\n",
      "            warning: manifest_maker: MANIFEST.in, line 4: path 'tmp/' cannot end with '/'\n",
      "      \n",
      "            warning: no previously-included files matching '*.cpp' found under directory 'thinc'\n",
      "            adding license file 'LICENSE'\n",
      "            writing manifest file 'thinc.egg-info\\SOURCES.txt'\n",
      "            C:\\Users\\Admin\\AppData\\Local\\Temp\\pip-build-env-2keqvg0y\\overlay\\Lib\\site-packages\\setuptools\\command\\build_py.py:207: _Warning: Package 'thinc.backends' is absent from the `packages` configuration.\n",
      "            !!\n",
      "      \n",
      "                    ********************************************************************************\n",
      "                    ############################\n",
      "                    # Package would be ignored #\n",
      "                    ############################\n",
      "                    Python recognizes 'thinc.backends' as an importable package[^1],\n",
      "                    but it is absent from setuptools' `packages` configuration.\n",
      "      \n",
      "                    This leads to an ambiguous overall configuration. If you want to distribute this\n",
      "                    package, please make sure that 'thinc.backends' is explicitly added\n",
      "                    to the `packages` configuration field.\n",
      "      \n",
      "                    Alternatively, you can also rely on setuptools' discovery methods\n",
      "                    (for example by using `find_namespace_packages(...)`/`find_namespace:`\n",
      "                    instead of `find_packages(...)`/`find:`).\n",
      "      \n",
      "                    You can read more about \"package discovery\" on setuptools documentation page:\n",
      "      \n",
      "                    - https://setuptools.pypa.io/en/latest/userguide/package_discovery.html\n",
      "      \n",
      "                    If you don't want 'thinc.backends' to be distributed and are\n",
      "                    already explicitly excluding 'thinc.backends' via\n",
      "                    `find_namespace_packages(...)/find_namespace` or `find_packages(...)/find`,\n",
      "                    you can try to use `exclude_package_data`, or `include-package-data=False` in\n",
      "                    combination with a more fine grained `package-data` configuration.\n",
      "      \n",
      "                    You can read more about \"package data files\" on setuptools documentation page:\n",
      "      \n",
      "                    - https://setuptools.pypa.io/en/latest/userguide/datafiles.html\n",
      "      \n",
      "      \n",
      "                    [^1]: For Python, any directory (with suitable naming) can be imported,\n",
      "                          even if it does not contain any `.py` files.\n",
      "                          On the other hand, currently there is no concept of package data\n",
      "                          directory, all directories are treated like packages.\n",
      "                    ********************************************************************************\n",
      "      \n",
      "            !!\n",
      "              check.warn(importable)\n",
      "            C:\\Users\\Admin\\AppData\\Local\\Temp\\pip-build-env-2keqvg0y\\overlay\\Lib\\site-packages\\setuptools\\command\\build_py.py:207: _Warning: Package 'thinc.extra' is absent from the `packages` configuration.\n",
      "            !!\n",
      "      \n",
      "                    ********************************************************************************\n",
      "                    ############################\n",
      "                    # Package would be ignored #\n",
      "                    ############################\n",
      "                    Python recognizes 'thinc.extra' as an importable package[^1],\n",
      "                    but it is absent from setuptools' `packages` configuration.\n",
      "      \n",
      "                    This leads to an ambiguous overall configuration. If you want to distribute this\n",
      "                    package, please make sure that 'thinc.extra' is explicitly added\n",
      "                    to the `packages` configuration field.\n",
      "      \n",
      "                    Alternatively, you can also rely on setuptools' discovery methods\n",
      "                    (for example by using `find_namespace_packages(...)`/`find_namespace:`\n",
      "                    instead of `find_packages(...)`/`find:`).\n",
      "      \n",
      "                    You can read more about \"package discovery\" on setuptools documentation page:\n",
      "      \n",
      "                    - https://setuptools.pypa.io/en/latest/userguide/package_discovery.html\n",
      "      \n",
      "                    If you don't want 'thinc.extra' to be distributed and are\n",
      "                    already explicitly excluding 'thinc.extra' via\n",
      "                    `find_namespace_packages(...)/find_namespace` or `find_packages(...)/find`,\n",
      "                    you can try to use `exclude_package_data`, or `include-package-data=False` in\n",
      "                    combination with a more fine grained `package-data` configuration.\n",
      "      \n",
      "                    You can read more about \"package data files\" on setuptools documentation page:\n",
      "      \n",
      "                    - https://setuptools.pypa.io/en/latest/userguide/datafiles.html\n",
      "      \n",
      "      \n",
      "                    [^1]: For Python, any directory (with suitable naming) can be imported,\n",
      "                          even if it does not contain any `.py` files.\n",
      "                          On the other hand, currently there is no concept of package data\n",
      "                          directory, all directories are treated like packages.\n",
      "                    ********************************************************************************\n",
      "      \n",
      "            !!\n",
      "              check.warn(importable)\n",
      "            C:\\Users\\Admin\\AppData\\Local\\Temp\\pip-build-env-2keqvg0y\\overlay\\Lib\\site-packages\\setuptools\\command\\build_py.py:207: _Warning: Package 'thinc.layers' is absent from the `packages` configuration.\n",
      "            !!\n",
      "      \n",
      "                    ********************************************************************************\n",
      "                    ############################\n",
      "                    # Package would be ignored #\n",
      "                    ############################\n",
      "                    Python recognizes 'thinc.layers' as an importable package[^1],\n",
      "                    but it is absent from setuptools' `packages` configuration.\n",
      "      \n",
      "                    This leads to an ambiguous overall configuration. If you want to distribute this\n",
      "                    package, please make sure that 'thinc.layers' is explicitly added\n",
      "                    to the `packages` configuration field.\n",
      "      \n",
      "                    Alternatively, you can also rely on setuptools' discovery methods\n",
      "                    (for example by using `find_namespace_packages(...)`/`find_namespace:`\n",
      "                    instead of `find_packages(...)`/`find:`).\n",
      "      \n",
      "                    You can read more about \"package discovery\" on setuptools documentation page:\n",
      "      \n",
      "                    - https://setuptools.pypa.io/en/latest/userguide/package_discovery.html\n",
      "      \n",
      "                    If you don't want 'thinc.layers' to be distributed and are\n",
      "                    already explicitly excluding 'thinc.layers' via\n",
      "                    `find_namespace_packages(...)/find_namespace` or `find_packages(...)/find`,\n",
      "                    you can try to use `exclude_package_data`, or `include-package-data=False` in\n",
      "                    combination with a more fine grained `package-data` configuration.\n",
      "      \n",
      "                    You can read more about \"package data files\" on setuptools documentation page:\n",
      "      \n",
      "                    - https://setuptools.pypa.io/en/latest/userguide/datafiles.html\n",
      "      \n",
      "      \n",
      "                    [^1]: For Python, any directory (with suitable naming) can be imported,\n",
      "                          even if it does not contain any `.py` files.\n",
      "                          On the other hand, currently there is no concept of package data\n",
      "                          directory, all directories are treated like packages.\n",
      "                    ********************************************************************************\n",
      "      \n",
      "            !!\n",
      "              check.warn(importable)\n",
      "            C:\\Users\\Admin\\AppData\\Local\\Temp\\pip-build-env-2keqvg0y\\overlay\\Lib\\site-packages\\setuptools\\command\\build_py.py:207: _Warning: Package 'thinc.tests.mypy.configs' is absent from the `packages` configuration.\n",
      "            !!\n",
      "      \n",
      "                    ********************************************************************************\n",
      "                    ############################\n",
      "                    # Package would be ignored #\n",
      "                    ############################\n",
      "                    Python recognizes 'thinc.tests.mypy.configs' as an importable package[^1],\n",
      "                    but it is absent from setuptools' `packages` configuration.\n",
      "      \n",
      "                    This leads to an ambiguous overall configuration. If you want to distribute this\n",
      "                    package, please make sure that 'thinc.tests.mypy.configs' is explicitly added\n",
      "                    to the `packages` configuration field.\n",
      "      \n",
      "                    Alternatively, you can also rely on setuptools' discovery methods\n",
      "                    (for example by using `find_namespace_packages(...)`/`find_namespace:`\n",
      "                    instead of `find_packages(...)`/`find:`).\n",
      "      \n",
      "                    You can read more about \"package discovery\" on setuptools documentation page:\n",
      "      \n",
      "                    - https://setuptools.pypa.io/en/latest/userguide/package_discovery.html\n",
      "      \n",
      "                    If you don't want 'thinc.tests.mypy.configs' to be distributed and are\n",
      "                    already explicitly excluding 'thinc.tests.mypy.configs' via\n",
      "                    `find_namespace_packages(...)/find_namespace` or `find_packages(...)/find`,\n",
      "                    you can try to use `exclude_package_data`, or `include-package-data=False` in\n",
      "                    combination with a more fine grained `package-data` configuration.\n",
      "      \n",
      "                    You can read more about \"package data files\" on setuptools documentation page:\n",
      "      \n",
      "                    - https://setuptools.pypa.io/en/latest/userguide/datafiles.html\n",
      "      \n",
      "      \n",
      "                    [^1]: For Python, any directory (with suitable naming) can be imported,\n",
      "                          even if it does not contain any `.py` files.\n",
      "                          On the other hand, currently there is no concept of package data\n",
      "                          directory, all directories are treated like packages.\n",
      "                    ********************************************************************************\n",
      "      \n",
      "            !!\n",
      "              check.warn(importable)\n",
      "            C:\\Users\\Admin\\AppData\\Local\\Temp\\pip-build-env-2keqvg0y\\overlay\\Lib\\site-packages\\setuptools\\command\\build_py.py:207: _Warning: Package 'thinc.tests.mypy.outputs' is absent from the `packages` configuration.\n",
      "            !!\n",
      "      \n",
      "                    ********************************************************************************\n",
      "                    ############################\n",
      "                    # Package would be ignored #\n",
      "                    ############################\n",
      "                    Python recognizes 'thinc.tests.mypy.outputs' as an importable package[^1],\n",
      "                    but it is absent from setuptools' `packages` configuration.\n",
      "      \n",
      "                    This leads to an ambiguous overall configuration. If you want to distribute this\n",
      "                    package, please make sure that 'thinc.tests.mypy.outputs' is explicitly added\n",
      "                    to the `packages` configuration field.\n",
      "      \n",
      "                    Alternatively, you can also rely on setuptools' discovery methods\n",
      "                    (for example by using `find_namespace_packages(...)`/`find_namespace:`\n",
      "                    instead of `find_packages(...)`/`find:`).\n",
      "      \n",
      "                    You can read more about \"package discovery\" on setuptools documentation page:\n",
      "      \n",
      "                    - https://setuptools.pypa.io/en/latest/userguide/package_discovery.html\n",
      "      \n",
      "                    If you don't want 'thinc.tests.mypy.outputs' to be distributed and are\n",
      "                    already explicitly excluding 'thinc.tests.mypy.outputs' via\n",
      "                    `find_namespace_packages(...)/find_namespace` or `find_packages(...)/find`,\n",
      "                    you can try to use `exclude_package_data`, or `include-package-data=False` in\n",
      "                    combination with a more fine grained `package-data` configuration.\n",
      "      \n",
      "                    You can read more about \"package data files\" on setuptools documentation page:\n",
      "      \n",
      "                    - https://setuptools.pypa.io/en/latest/userguide/datafiles.html\n",
      "      \n",
      "      \n",
      "                    [^1]: For Python, any directory (with suitable naming) can be imported,\n",
      "                          even if it does not contain any `.py` files.\n",
      "                          On the other hand, currently there is no concept of package data\n",
      "                          directory, all directories are treated like packages.\n",
      "                    ********************************************************************************\n",
      "      \n",
      "            !!\n",
      "              check.warn(importable)\n",
      "            copying thinc\\__init__.pxd -> build\\lib.win-amd64-cpython-311\\thinc\n",
      "            copying thinc\\py.typed -> build\\lib.win-amd64-cpython-311\\thinc\n",
      "            copying thinc\\backends\\linalg.cpp -> build\\lib.win-amd64-cpython-311\\thinc\\backends\n",
      "            copying thinc\\backends\\numpy_ops.cpp -> build\\lib.win-amd64-cpython-311\\thinc\\backends\n",
      "            copying thinc\\extra\\search.cpp -> build\\lib.win-amd64-cpython-311\\thinc\\extra\n",
      "            copying thinc\\layers\\sparselinear.cpp -> build\\lib.win-amd64-cpython-311\\thinc\\layers\n",
      "            copying thinc\\backends\\__init__.pxd -> build\\lib.win-amd64-cpython-311\\thinc\\backends\n",
      "            copying thinc\\backends\\_custom_kernels.cu -> build\\lib.win-amd64-cpython-311\\thinc\\backends\n",
      "            copying thinc\\backends\\_murmur3.cu -> build\\lib.win-amd64-cpython-311\\thinc\\backends\n",
      "            copying thinc\\backends\\linalg.pxd -> build\\lib.win-amd64-cpython-311\\thinc\\backends\n",
      "            copying thinc\\backends\\linalg.pyx -> build\\lib.win-amd64-cpython-311\\thinc\\backends\n",
      "            copying thinc\\backends\\numpy_ops.pxd -> build\\lib.win-amd64-cpython-311\\thinc\\backends\n",
      "            copying thinc\\backends\\numpy_ops.pyx -> build\\lib.win-amd64-cpython-311\\thinc\\backends\n",
      "            copying thinc\\extra\\__init__.pxd -> build\\lib.win-amd64-cpython-311\\thinc\\extra\n",
      "            copying thinc\\extra\\search.pxd -> build\\lib.win-amd64-cpython-311\\thinc\\extra\n",
      "            copying thinc\\extra\\search.pyx -> build\\lib.win-amd64-cpython-311\\thinc\\extra\n",
      "            copying thinc\\layers\\sparselinear.pyx -> build\\lib.win-amd64-cpython-311\\thinc\\layers\n",
      "            copying thinc\\extra\\tests\\c_test_search.pyx -> build\\lib.win-amd64-cpython-311\\thinc\\extra\\tests\n",
      "            creating build\\lib.win-amd64-cpython-311\\thinc\\tests\\mypy\\configs\n",
      "            copying thinc\\tests\\mypy\\configs\\mypy-default.ini -> build\\lib.win-amd64-cpython-311\\thinc\\tests\\mypy\\configs\n",
      "            copying thinc\\tests\\mypy\\configs\\mypy-plugin.ini -> build\\lib.win-amd64-cpython-311\\thinc\\tests\\mypy\\configs\n",
      "            creating build\\lib.win-amd64-cpython-311\\thinc\\tests\\mypy\\outputs\n",
      "            copying thinc\\tests\\mypy\\outputs\\fail-no-plugin.txt -> build\\lib.win-amd64-cpython-311\\thinc\\tests\\mypy\\outputs\n",
      "            copying thinc\\tests\\mypy\\outputs\\fail-plugin.txt -> build\\lib.win-amd64-cpython-311\\thinc\\tests\\mypy\\outputs\n",
      "            copying thinc\\tests\\mypy\\outputs\\success-no-plugin.txt -> build\\lib.win-amd64-cpython-311\\thinc\\tests\\mypy\\outputs\n",
      "            copying thinc\\tests\\mypy\\outputs\\success-plugin.txt -> build\\lib.win-amd64-cpython-311\\thinc\\tests\\mypy\\outputs\n",
      "            running build_ext\n",
      "            error: Microsoft Visual C++ 14.0 or greater is required. Get it with \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
      "            [end of output]\n",
      "      \n",
      "        note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "        ERROR: Failed building wheel for thinc\n",
      "      Failed to build thinc\n",
      "      ERROR: Could not build wheels for thinc, which is required to install pyproject.toml-based projects\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "error: subprocess-exited-with-error\n",
      "\n",
      "× pip subprocess to install build dependencies did not run successfully.\n",
      "│ exit code: 1\n",
      "╰─> See above for output.\n",
      "\n",
      "note: This error originates from a subprocess, and is likely not a problem with pip.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Saving to output directory: output\u001b[0m\n",
      "\u001b[38;5;4mℹ Using CPU\u001b[0m\n",
      "\u001b[1m\n",
      "=========================== Initializing pipeline ===========================\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\spacy\\language.py\", line 1327, in initialize\n",
      "    init_vocab(\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\spacy\\training\\initialize.py\", line 142, in init_vocab\n",
      "    load_vectors_into_model(nlp, vectors)\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\spacy\\training\\initialize.py\", line 164, in load_vectors_into_model\n",
      "    vectors_nlp = load_model(name, vocab=nlp.vocab, exclude=exclude)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\spacy\\util.py\", line 472, in load_model\n",
      "    raise IOError(Errors.E050.format(name=name))\n",
      "OSError: [E050] Can't find model 'fr_core_news_lg'. It doesn't seem to be a Python package or a valid path to a data directory.\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\spacy\\__main__.py\", line 4, in <module>\n",
      "    setup_cli()\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\spacy\\cli\\_util.py\", line 87, in setup_cli\n",
      "    command(prog_name=COMMAND)\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\click\\core.py\", line 1157, in __call__\n",
      "    return self.main(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\typer\\core.py\", line 778, in main\n",
      "    return _main(\n",
      "           ^^^^^^\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\typer\\core.py\", line 216, in _main\n",
      "    rv = self.invoke(ctx)\n",
      "         ^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\click\\core.py\", line 1688, in invoke\n",
      "    return _process_result(sub_ctx.command.invoke(sub_ctx))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\click\\core.py\", line 1434, in invoke\n",
      "    return ctx.invoke(self.callback, **ctx.params)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\click\\core.py\", line 783, in invoke\n",
      "    return __callback(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\typer\\main.py\", line 683, in wrapper\n",
      "    return callback(**use_params)  # type: ignore\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\spacy\\cli\\train.py\", line 54, in train_cli\n",
      "    train(config_path, output_path, use_gpu=use_gpu, overrides=overrides)\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\spacy\\cli\\train.py\", line 81, in train\n",
      "    nlp = init_nlp(config, use_gpu=use_gpu)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\spacy\\training\\initialize.py\", line 95, in init_nlp\n",
      "    nlp.initialize(lambda: train_corpus(nlp), sgd=optimizer)\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\spacy\\language.py\", line 1331, in initialize\n",
      "    raise IOError(Errors.E884.format(vectors=I[\"vectors\"]))\n",
      "OSError: [E884] The pipeline could not be initialized because the vectors could not be found at 'fr_core_news_lg'. If your pipeline was already initialized/trained before, call 'resume_training' instead of 'initialize', or initialize only the components that are new.\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy train config.cfg --output ./output --paths.train ./train.spacy --paths.dev ./test.spacy \n",
    "#--gpu-id 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Integrating the Whisper ASR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not path_exists('transcripts'):\n",
    "    !mkdir transcripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "YouTubeID = 'xOZM-1p-jAk' # ¿Qué es la enfermedad de Crohn y cómo detectarla?\n",
    "OutputFile = 'test_audio_youtube_5.m4a'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not path_exists(OutputFile):\n",
    "    !youtube-dl -o \n",
    "YouTubeID --extract-audio --restrict-filenames -f 'bestaudio[ext=m4a]'\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_whisper = whisper.load_model(\"small\")\n",
    "result = model_whisper.transcribe(OutputFile)\n",
    "print(result[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy_transformers\n",
    "nlp1 = spacy.load(r\"/content/output/model-best\") #load the best model\n",
    "\n",
    "doc = nlp1(result[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ents = dict_specialty_keywords_fr.keys()\n",
    "\n",
    "\n",
    "import random\n",
    "random.seed(38)\n",
    "\n",
    "colors = {}\n",
    "for j in ents:\n",
    "    rand_colors = [\"#\"+''.join([random.choice('ABCDEF0123456789') for i in range(6)])]\n",
    "    colors[j] = rand_colors[0]\n",
    "\n",
    "options = {\"ents\": list(ents), \"colors\": colors }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy.displacy.render(doc, style='ent',jupyter=True, options=options)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
